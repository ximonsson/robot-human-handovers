\section{Conclusions}

In this work we introduced a model for a system to learn proper handover features for objects through observations of handovers performed by humans. We showed that we can classify objects depending on defined settings for a handover and with good results identify an object's class from images, with good results for object's that were present in the observations as well as for novel objects.

Based on the features extracted from the handovers, the system is able to cluster the objects and classify them. A handover class will then be defined by a set of features that are to be applied to an object when handing it over. Before clustering we performed PCA to identify the dominant features which were: rotation of the object, direction of vector between center of mass of the object and center grasp region in y-axis and ratio of object covered by the grasp. Two main classes were identified, were one class contained larger cylindrical objects, used for containing for example liquids (cup, glass, bottle, etc.) were handover in a similar fashion, while objects that are thinner and have a distinct usage area (hammer, pen, scissors) had a different set of features. Outliers in the training were clustered into their own classes (cutters, box and tube) that contained unique attributes for these objects.

Color images were then taken of the objects from different angles, and the blue channel was replaced with the depth image, and used for training a CNN to recognize the objects and output their respective handover class. The classes that contained only one object were discarded as they would not contain enough data to properly train to identify them. The resulting network was then tested on a different set of images of objects that resembled the training set and manually labeled after the two main classes previously created.

Mean performance for our network is 90\% with a standard deviation of 2\%. We not that the network had a hard time learning to recognize objects made of glass, probably because the issues with the infrared sensor in the depth camera refracting towards the material. Objects in the same class but made of other material had results close to 100\% which confirms this theory. Looking at the confusion matrix we can observe that the network became more biased for the first class than the second, which is likely due to the fact that the two classes were not completely balanced between the objects, suggesting that one class had more versatile features to learn from. In overall the results are quite good considering the small dataset to train on and the artifacts that are created with the depth camera, and with a larger amount of objects and images better results could have probably been achieved. More ideas on how to improve this model are presented in section \ref{sec:future-work}.


\section{Future Work}
\label{sec:future-work}

The results of this work show that a system is able to autonomisly learn to handover objects and adapt to novel ones, but some caveats do exist for it to become truly autonomous. The largest problem is object detection, which is a challenging field within computer vision. This work took help of AprilTags to locate and track the objects but in a live environment these tags do not exist on all objects and the system would need a better way of recognizing the object in the handover scene to extract data about the handover. Some work show good success using alternatives such as point cloud libraries (\parencite{Chan2015a}). Object detection using point clouds was tried in this work before using AprilTags, but with very poor results. This is something that would need more work to try and implement better without the help of the tags.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/conclusion/awkward_handover_frame.jpg}
	\caption{Some grasps could become unnatural because of the AprilTags}
	\label{fig:fw_handover_awkward}
\end{figure}

Handover classes from clustering are however not enough to tell a robot how to grasp an object when handing it over. As seen in the results when the settings of one class are applied to an object the grasp region can become outside of it, though rotation and direction from center of object are correct. Future work would have to include calculating more precisely where a robot can grasp the object when handing it over given the data from the handover class.

Considering the amount of data that is needed for successfully training a CNN to perform object classification one can wonder if it is the best choice for such an application. In this work we needed to take help of pre-trained weights to classify the objects successfully, but without we get very poor accuracy. If trained from the scratch the system would need to observe quite many different objects to classify them correctly, also know when different objects are the same class. Future work would be to investigate results using other learning algorithms for images, or feature extraction from the objects and learn from them instead.
