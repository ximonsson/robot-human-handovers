@article{Grigore2013,
abstract = {The development of trustworthy human-assistive robots is a challenge that goes beyond the traditional boundaries of engineering. Essential components of trustworthiness are safety, predictability and usefulness. In this paper we demonstrate that the integration of joint action understanding from human-human interaction into the human-robot context can significantly improve the success rate of robot-to-human object handover tasks. We take a two layer approach. The first layer handles the physical aspects of the handover. The robot's decision to release the object is informed by a Hidden Markov Model that estimates the state of the handover. Inspired by human-human handover observations, we then introduce a higher-level cognitive layer that models behaviour characteristic for a human user in a handover situation. In particular, we focus on the inclusion of eye gaze / head orientation into the robot's decision making. Our results demonstrate that by integrating these non-verbal cues the success rate of robot-to-human handovers can be significantly improved, resulting in a more robust and therefore safer system.},
author = {Grigore, Elena Corina and Eder, Kerstin and Pipe, Anthony G. and Melhuish, Chris and Leonards, Ute},
doi = {10.1109/IROS.2013.6697021},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grigore et al. - 2013 - Joint action understanding improves robot-to-human object handover.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4622--4629},
title = {{Joint action understanding improves robot-to-human object handover}},
year = {2013}
}
@article{Chan2014,
abstract = {We present a method for enabling robots to determine appropriate grasp configurations for handovers - i.e., where to grasp, and how to orient an object when handing it over. In our method, a robot first builds a knowledge base by observing demonstrations of how certain objects are used and their proper handover grasp configurations. Objects in the knowledge base are then organized based on their movements and inter-object interaction features. The key point in this process is that similarity in affordances should be recognized. When subsequently asked to handover an object, the robot then computes an appropriate grasp configuration based on the object's recognized affordances. Experimental results show that our method was able to differentiate and group together objects according to their affordances. Furthermore, when given a new object, our method was able to generalize data in the knowledge base and determine an appropriate grasp configuration. {\&}copy; 2014 IEEE.},
author = {Chan, Wesley P. and Kakiuchi, Yohei and Okada, Kei and Inaba, Masayuki},
doi = {10.1109/IROS.2014.6942733},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan et al. - 2014 - Determining proper grasp configurations for handovers through observation of object movement patterns and inter-obj.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Iros},
pages = {1355--1360},
title = {{Determining proper grasp configurations for handovers through observation of object movement patterns and inter-object interactions during usage}},
volume = {8656},
year = {2014}
}
@article{Szegedy2014,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Sato2002,
abstract = { In order to transmit, share and store human knowledge, it is important for a robot to be able to acquire task models and skills by observing human actions. Since vision plays an important role for observation, we propose a technique for measuring the position and posture of objects and hands in 3-dimensional space at high speed and with high precision by vision. Next, we show a framework for the analysis and description of a task by using an object functions as elements.},
author = {Sato, Y. and Bernardin, K. and Kimura, H. and Ikeuchi, K.},
doi = {10.1109/IRDS.2002.1043898},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sato et al. - 2002 - Task analysis based on observing hands and objects by vision.pdf:pdf},
isbn = {0-7803-7398-7},
journal = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
number = {October},
pages = {5--10},
title = {{Task analysis based on observing hands and objects by vision}},
volume = {2},
year = {2002}
}
@article{Chan2015,
abstract = {To enable robots to learn handover orientations from observing natural handovers, we conduct a user study to measure and compare natural handover orientations with giver-centered and receiver-centered handover orientations for twenty common objects. We use a distance minimization approach to compute mean handover orientations. We posit that, computed means of receiver-centered orientations could be used by robot givers to achieve more efficient and socially acceptable handovers. Furthermore, we introduce the notion of affordance axes for comparing handover orientations, and offer a definition for computing them. Observable patterns were found in receiver-centered handover orientations. Comparisons show that depending on the object, natural handover orientations may not be receiver-centered; thus, robots may need to distinguish between good and bad handover orientations when learning from natural handovers.},
author = {Chan, Wesley P. and Pan, Matthew K.X.J. and Croft, Elizabeth A. and Inaba, Masayuki},
doi = {10.1109/IROS.2015.7353106},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan et al. - 2015 - Characterization of handover orientations used by humans for efficient robot to human handovers.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Atmospheric measurements,Collaboration,Handover,Particle measurements,Receivers,Robots},
pages = {1--6},
title = {{Characterization of handover orientations used by humans for efficient robot to human handovers}},
volume = {2015-Decem},
year = {2015}
}
@article{Koene2014,
abstract = {In current society there is a growing call for robotic platforms designed to provide direct assistance to humans within the near future. An essential requirement, if robots are to become accepted as service providers that interact directly with humans, is that the human-robot interactions must not only be safe and reliable, but also produce a satisfying user experience. Here we report results from a study on human-robot object handovers in which we show that timing aspect of the robot response outweighed spatial precision for determining user experience ratings.},
author = {Koene, Ansgar and Remazeilles, Anthony and Prada, Miguel and Garzo, Ainara and Puerto, Mildred and Endo, Satoshi and Wing, Alan M},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koene et al. - 2014 - Relative importance of spatial and temporal precision for user satisfaction in human-robot object handover interac.pdf:pdf},
journal = {Third International Symposium on New Frontiers in Human-Robot Interaction},
number = {October 2015},
title = {{Relative importance of spatial and temporal precision for user satisfaction in human-robot object handover interactions}},
year = {2014}
}
@article{Miller2003,
author = {Miller, Andrew T and Knoop, Steffen and Christensen, Henrik I and Allen, Peter K},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller et al. - 2003 - Automatic Grasp Planning Using Shape Primitives.pdf:pdf},
isbn = {0780377362},
journal = {Proceedings of IEEE International Conference on Robotics and Automation},
pages = {1824--1829},
title = {{Automatic Grasp Planning Using Shape Primitives}},
year = {2003}
}
@article{Sahbani2012,
abstract = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger-object contact interactions [7] or robot hand design and their control [1]. Robot grasp synthesis algorithms have been reviewed in [63], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
author = {Sahbani, Anis and El-Khoury, Sahar and Bidaud, Philippe},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahbani, El-Khoury, Bidaud - 2012 - An Overview of 3D Object Grasp Synthesis Algorithms An Overview of 3D Object Grasp Synthesis Algorit.pdf:pdf},
keywords = {()},
number = {3},
pages = {326--336},
title = {{An Overview of 3D Object Grasp Synthesis Algorithms An Overview of 3D Object Grasp Synthesis Algorithms. Robotics and Autonomous Systems}},
url = {http://hal.upmc.fr/file/index/docid/731127/filename/Survey{\_}SI{\_}RAS{\_}1.pdf{\%}0Ahttps://hal.archives-ouvertes.fr/hal-00731127},
volume = {60},
year = {2012}
}
@article{Moon2014,
abstract = {In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 na{\"{i}}ve subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turn- taking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.},
author = {Moon, AJung and Troniak, Daniel M. and Gleeson, Brian and Pan, Matthew K. X. J. and Zeng, Minhua and Blumer, Benjamin A. and MacLean, Karon and Croft, Elizabeth A.},
doi = {10.1145/2559636.2559656},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moon et al. - 2014 - Meet me where i'm gazing how shared attention gaze affects human-robot handover timing.pdf:pdf},
isbn = {9781450326582},
issn = {21672148},
journal = {Human-Robot Interaction},
keywords = {gaze,handover,head gaze,human-robot communication,nonverbal communication,turn-taking},
mendeley-tags = {handover},
pages = {334--341},
title = {{Meet me where i'm gazing: how shared attention gaze affects human-robot handover timing}},
url = {http://dl.acm.org/citation.cfm?id=2559636.2559656},
year = {2014}
}
@article{Mainprice2012,
abstract = {For a versatile human-assisting mobile-manipulating robot such as the PR2, handing over objects to humans in possibly cluttered workspaces is a key capability. In this paper we investigate the motion planning of handovers while accounting for the human mobility. We treat the human motion as part of the planning problem thus enabling to find broader type of handing strategies. We formalize the problem and propose an algorithmic solution taking into account the HRI constraints induced by the human receiver presence. Simulation results with the PR2 robot illustrate the efficacy of the approach.},
author = {Mainprice, Jim and Gharbi, Mamoun and Simeon, Thierry and Alami, Rachid},
doi = {10.1109/ROMAN.2012.6343844},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mainprice et al. - 2012 - Sharing effort in planning human-robot handover tasks.pdf:pdf},
isbn = {9781467346054},
issn = {1944-9445},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {764--770},
title = {{Sharing effort in planning human-robot handover tasks}},
year = {2012}
}
@article{Huber2008a,
abstract = {Joint-action is one of the key research areas in robotics and especially important in physical human-robot interaction. The two main criteria for robots, which should be integrated in everyday life, are safety and efficiency. Therefore, it is of particular interest to understand how humans work together in order to transfer the resulting facts from these studies to direct human-robot interaction. In this work, we investigate a simple case of physical human-robot interaction, i.e. the handing over of small objects from a robot to the human. Experiments, in which six cubes were handed over from the robot to the human, were performend with two different robot systems, a robot arm in a humanoid set-up and a typical industrial set-up. Two different velocity profiles were integrated in the robot systems, a trapezoidal velocity profile in joint coordinates and a human inspired minimum jerk profile in cartesian coordinates. In both set-ups the use of the minimum jerk profile lead to shorter reaction times of the humans for the interaction. The humanoid setup showed with both profiles shorter reaction times than the industrial setup. It was also investigated in the experiments, whether the human body position adopts during the experiments to an optimal position for the hand-over. During the experiments the body spatial position stayed largely invariant, which indicates, that the subjects were not frightened and felt comfortable with the given hand over position. The result of our experiments along with the given comparison to natural human-human behaviour provides a solid basis for more efficiency of collaboration of humans and assistive robot systems},
author = {Huber, Markus and Lenz, Claus and Rickert, Markus and Knoll, Alois and Brandt, Thomas and Glasauer, Stefan},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huber et al. - 2008 - Human preferences in industrial human-robot interactions.pdf:pdf},
journal = {Proceedings of the International Workshop on Cognition for Technical Systems},
pages = {1--6},
title = {{Human preferences in industrial human-robot interactions}},
url = {http://www6.in.tum.de/Main/Publications/Huber2008b.pdf},
year = {2008}
}
@article{Song2015,
abstract = {Grasping and manipulating everyday objects in a goal-directed manner is an important ability of a service robot. The robot needs to reason about task requirements and ground these in the sensorimotor information. Grasping and interaction with objects are challenging in real-world scenarios, where sensorimotor uncertainty is prevalent. This paper presents a probabilistic framework for the representation and modeling of robot-grasping tasks. The framework consists of Gaussian mixture models for generic data discretization, and discrete Bayesian networks for encoding the probabilistic relations among various task-relevant variables, including object and action features as well as task constraints. We evaluate the framework using a grasp database generated in a simulated environment including a human and two robot hand models. The generative modeling approach allows the prediction of grasping tasks given uncertain sensory data, as well as object and grasp selection in a task-oriented manner. Furthermore, the graphical model framework provides insights into dependencies between variables and features relevant for object grasping.},
author = {Song, Dan and Ek, Carl Henrik and Huebner, Kai and Kragic, Danica},
doi = {10.1109/TRO.2015.2409912},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2015 - Task-Based Robot Grasp Planning Using Probabilistic Inference.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Cognitive human-robot interaction,grasping,learning and adaptive systems,probabilistic graphical models,recognition},
number = {3},
pages = {546--561},
title = {{Task-Based Robot Grasp Planning Using Probabilistic Inference}},
volume = {31},
year = {2015}
}
@article{Suay2015,
abstract = {We present a novel approach for generating candidate handover positions for human receivers. The object handover problem involves several variables and is under-constrained. The sheer number of possibilities makes it nontrivial to find a good object handover position for existing algorithms. To address this problem, we introduce the use of a custom biomechanical model in a novel algorithm for generating a handover position. The biomechanical model serves as a tool for approximating the static strength a human receiver needs to exert in order to receive an object. A mobile manipulator can use the output of our algorithm when calculating a target position for handing over an object to a human. We show preliminary results involving electromyography data and joint moment values, as well as a proof-of-concept implementation with a mobile manipulator to fetch a toolbox to a receiver.},
author = {Suay, Halit Bener and Sisbot, Emrah Akin},
doi = {10.1109/ICRA.2015.7139724},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suay, Sisbot - 2015 - A position generation algorithm utilizing a biomechanical model for robot-human object handover.pdf:pdf},
isbn = {VO  -},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {3776--3781},
title = {{A position generation algorithm utilizing a biomechanical model for robot-human object handover}},
volume = {2015-June},
year = {2015}
}
@article{Schettino2003,
abstract = {Normal subjects gradually preshape their hands during a grasping movement in order to conform the hand to the shape of a target object. The evolution of hand preshaping may depend on visual feedback about arm and hand position as well as on target shape and location at specific times during the movement. The present study manipulated object shape in order to produce differentiable patterns of finger placement along two orthogonal "dimensions" (flexion/extension and abduction/adduction), and manipulated the amount of available visual information during a grasp. Normal subjects were asked to reach to and grasp a set of objects presented in a randomized fashion at a fixed spatial location in three visual feedback conditions: Full Vision (both hand and target visible), Object Vision (only the object was visible but not the hand) and No Vision (vision of neither the hand nor the object during the movement). Flexion/extension angles of the metacarpophalangeal and proximal interphalangeal joints of the index, ring, middle and pinkie fingers as well as the abduction/adduction angles between the index-middle and middle-ring fingers were recorded. Kinematic analysis revealed that as visual feedback was reduced, movement duration increased and time to peak aperture of the hand decreased, in accord with previously reported studies. Analysis of the patterns of joint flexion/extension and abduction/adduction per object shape revealed that preshaping based on the abduction/adduction dimension occurred early during the reach for all visual feedback conditions (approximately 45{\%} of normalized movement time). This early preshaping across visual feedback conditions suggests the existence of mechanisms involved in the selection of basic hand configurations. Furthermore, while configuration changes in the flexion/extension dimension resulting in well-defined hand configurations occurred earlier during the movement in the Object Vision and No Vision conditions (45{\%}), those in the Full Vision condition were observed only after 75{\%} of the movement, as the moving hand entered the central region of the visual field. The data indicate that there are at least two control mechanisms at work during hand preshaping, an early predictive phase during which grip selection is attained regardless of availability of visual feedback and a late responsive phase during which subjects may use visual feedback to optimize their grasp.},
author = {Schettino, Luis F. and Adamovich, Sergei V. and Poizner, Howard},
doi = {10.1007/s00221-003-1435-3},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schettino, Adamovich, Poizner - 2003 - Effects of object shape and visual feedback on hand configuration during grasping.pdf:pdf},
isbn = {0014-4819 (Print)},
issn = {00144819},
journal = {Experimental Brain Research},
keywords = {Hand preshaping,Prehension,Visual feedback,Visuomotor control},
number = {2},
pages = {158--166},
pmid = {12783144},
title = {{Effects of object shape and visual feedback on hand configuration during grasping}},
volume = {151},
year = {2003}
}
@article{Lenz2015,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.},
archivePrefix = {arXiv},
arxivId = {1301.3592},
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
doi = {10.1177/0278364914549607},
eprint = {1301.3592},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lenz, Lee, Saxena - 2015 - Deep learning for detecting robotic grasps.pdf:pdf},
isbn = {9789810739379},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {3D feature learning,Baxter,PR2,RGB-D multi-modal data,Robotic grasping,deep learning},
number = {4-5},
pages = {705--724},
pmid = {19352402},
title = {{Deep learning for detecting robotic grasps}},
volume = {34},
year = {2015}
}
@article{Strabala2013,
abstract = {A handover is a complex collaboration, where actors coordinate in time and space to transfer control of an object. This coordination comprises two processes: the physical process of moving to get close enough to transfer the object, and the cognitive process of exchanging information to guide the transfer. Despite this complexity, we humans are capable of performing handovers seamlessly in a wide variety of situations, even when unexpected. This suggests a common procedure that guides all handover interactions. Our goal is to codify that procedure. To that end, we first study how people hand over objects to each other in order to understand their coordination process and the signals and cues that they use and observe with their partners. Based on these studies, we propose a coordination structure for human--robot handovers that considers the physical and social-cognitive aspects of the interaction separately. This handover structure describes how people approach, reach out their hands, and transfer objects while simultaneously coordinating the what, when, and where of handovers: to agree that the handover will happen (and with what object), to establish the timing of the handover, and to decide the configuration at which the handover will occur. We experimentally evaluate human-robot handover behaviors that exploit this structure and offer design implications for seamless human-robot handover interactions.},
author = {Strabala, Kyle Wayne and Lee, Min Kyung and Dragan, Anca Diana and Forlizzi, Jodi Lee and Srinivasa, Siddhartha and Cakmak, Maya and Micelli, Vincenzo},
doi = {10.5898/JHRI.2.1.Strabala},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Strabala et al. - 2013 - Towards Seamless Human-Robot Handovers.pdf:pdf},
isbn = {21630364},
issn = {21630364},
journal = {Journal of Human-Robot Interaction},
keywords = {handover,joint activity,physical human,robot interaction,signaling},
number = {1},
pages = {112--132},
title = {{Towards Seamless Human-Robot Handovers}},
url = {http://www.humanrobotinteraction.org/journal/index.php/HRI/article/view/114},
volume = {2},
year = {2013}
}
@article{Huber2008,
abstract = {In many future joint-action scenarios, humans and robots will have to interact physically in order to successfully cooperate. Ideally, seamless human-robot interaction should not require training for the human, but should be intuitively simple. Nonetheless, seamless interaction and cooperation involve some degree of learning and adaptation. Here, we report on a simple case of physical human-robot interaction, a hand-over task. Even such a basic task as manually handing over an object from one agent to another requires that both partners agree upon certain basic prerequisites and boundary conditions. While some of them are negotiated explicitly, e.g. by verbal communication, others are determined indirectly and adaptively in the course of the cooperation. In the present study, we compared human-human hand-over interaction with the same task done by a robot and a human. To evaluate the importance of biological motion, the robot human interaction was tested with two different velocity profiles: a conventional trapezoidal velocity profile in joint coordinates and a minimum-jerk profile of the end-effector. Our results show a significantly shorter reaction time for minimum jerk profiles, which decreased over the first three hand-overs. The results of our comparison provide the background for implementing effective joint-action strategies in humanoid robot systems.},
author = {Huber, Markus and Rickert, Markus and Knoll, Alois and Brandt, Thomas and Glasauer, Stefan},
doi = {10.1109/ROMAN.2008.4600651},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huber et al. - 2008 - Human-robot interaction in handing-over tasks.pdf:pdf},
isbn = {9781424422135},
issn = {1944-9445},
journal = {Proceedings of the 17th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN},
number = {November 2015},
pages = {107--112},
title = {{Human-robot interaction in handing-over tasks}},
year = {2008}
}
@article{Kim2004,
abstract = {For home-service robots one of the most common interactions with humans is the handing over of objects using arms and hands of anthropomorphic robots. We propose advanced methods of determination of grasp sites for handover operations that incorporate especially etiquettes which factors include object shapes, object functions, and safety. We also address different operations for handover: one-handed handover with one grasp, two-step handover with midair re-grasp, and two-handed handover. We show the effectiveness of our algorithm for interaction between humans and robots with graphical simulations.},
author = {Kim, Jinsul and Park, Jihwan and Hwang, Yong and Lee, Manjai},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2004 - Advanced Grasp Planning for Handover Operation Between Human and Robot Three Handover Methods in Esteem Etiquettes U.pdf:pdf},
journal = {2nd International Conference on Autonomous Robots and Agents},
keywords = {dual-arms-and-hands,etiquette,grasp-planning,handover-operation,humanoid-home-service-robot,motion-planning},
number = {c},
pages = {34--39},
title = {{Advanced Grasp Planning for Handover Operation Between Human and Robot: Three Handover Methods in Esteem Etiquettes Using Dual Arms and Hands of Home-Service Robot}},
url = {citeulike-article-id:6221504},
year = {2004}
}
@article{Aleotti2012,
abstract = {A method for robot to human object hand-over is presented that takes into account user comfort. Comfort is addressed by serving the object to facilitate user's convenience. The object is delivered so that the most appropriate part is oriented towards the person interacting with the robot. This approach, aimed at contributing to the development of socially aware robots, has not been considered in previous works. The robot system also supports sensory-motor skills like object and people detection, robot grasping and motion planning. The experimental setup consists of a six degrees of freedom robot arm with both an eye-in-hand laser scanner and a fixed range sensor. The user interacting with the robot can assume an arbitrary position in front of the robot. Experiments are reported from a user study.},
author = {Aleotti, Jacopo and Micelli, Vincenzo and Caselli, Stefano},
doi = {10.1109/ROMAN.2012.6343845},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aleotti, Micelli, Caselli - 2012 - Comfortable robot to human object hand-over.pdf:pdf},
isbn = {9781467346054},
issn = {18754805},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {771--776},
title = {{Comfortable robot to human object hand-over}},
year = {2012}
}
@article{Jiang2011,
abstract = {Given an image and an aligned depth map of an object, our goal is to estimate the full 7-dimensional gripper configuration{\&}{\#}x2014;its 3D location, 3D orientation and the gripper opening width. Recently, learning algorithms have been successfully applied to grasp novel objects{\&}{\#}x2014;ones not seen by the robot before. While these approaches use low-dimensional representations such as a {\&}{\#}x2018;grasping point{\&}{\#}x2019; or a {\&}{\#}x2018;pair of points{\&}{\#}x2019; that are perhaps easier to learn, they only partly represent the gripper configuration and hence are sub-optimal. We propose to learn a new {\&}{\#}x2018;grasping rectangle{\&}{\#}x2019; representation: an oriented rectangle in the image plane. It takes into account the location, the orientation as well as the gripper opening width. However, inference with such a representation is computationally expensive. In this work, we present a two step process in which the first step prunes the search space efficiently using certain features that are fast to compute. For the remaining few cases, the second step uses advanced features to accurately select a good grasp. In our extensive experiments, we show that our robot successfully uses our algorithm to pick up a variety of novel objects.},
author = {Jiang, Yun and Moseson, Stephen and Saxena, Ashutosh},
doi = {10.1109/ICRA.2011.5980145},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Moseson, Saxena - 2011 - Efficient grasping from RGBD images Learning using a new rectangle representation.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3304--3311},
pmid = {5980145},
title = {{Efficient grasping from RGBD images: Learning using a new rectangle representation}},
year = {2011}
}
@article{Yang,
author = {Yang, Yezhou and Ferm, Cornelia},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Ferm - Unknown - Grasp Type Revisited A Modern Perspective on A Classical Feature for Vision.pdf:pdf},
number = {c},
title = {{Grasp Type Revisited : A Modern Perspective on A Classical Feature for Vision}}
}
@article{Admoni2014,
abstract = {As assistive robots become popular in factories and homes, there is greater need for natural, multi-channel communication during collaborative manipulation tasks. Non-verbal communication such as eye gaze can provide information without overloading more taxing channels like speech. However, certain collaborative tasks may draw attention away from these subtle communication modalities. For instance, robot-to-human handovers are primarily manual tasks, and human attention is therefore drawn to robot hands rather than to robot faces during handovers. In this paper, we show that a simple manipulation of a robot's handover behavior can significantly increase both awareness of the robot's eye gaze and compliance with that gaze. When eye gaze communication occurs during the robot's release of an object, delaying object release until the gaze is finished draws attention back to the robot's head, which increases conscious perception of the robot's communication. Furthermore, the handover delay increases peoples' compliance with the robot's communication over a non-delayed handover, even when compliance results in counterintuitive behavior.},
author = {Admoni, Henny and Dragan, Anca and Srinivasa, Siddhartha S. and Scassellati, Brian},
doi = {10.1145/2559636.2559682},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Admoni et al. - 2014 - Deliberate delays during robot-to-human handovers improve compliance with gaze communication.pdf:pdf},
isbn = {9781450326582},
issn = {21672148},
journal = {Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction - HRI '14},
pages = {49--56},
title = {{Deliberate delays during robot-to-human handovers improve compliance with gaze communication}},
url = {http://dl.acm.org/citation.cfm?doid=2559636.2559682},
year = {2014}
}
@article{Huang2015,
abstract = {Handovers of objects are critical interactions that frequently arise in physical collaborations. In such interactions, humans naturally monitor the pace and workload of their partners and adapt their handovers accordingly. In this paper, we investigate how robots designed to engage in physical collaborations may achieve similar adaptivity in performing handovers. To that end, we collected and analyzed data from human dyads performing a common household task unloading a dish rack where receivers had different levels of task demands. We identified two coordination strategies that enabled givers to adapt to receivers task demands. We then formulated and implemented these strategies on a robotic manipulator. The implemented autonomous system was evaluated in a human-robot interaction study against two baselines that use proactive and reactive coordination methods. The results show a tradeoff between team performance and user experience when human receivers had greater task demands. In particular, the proactive method provided the greatest levels of team performance but offered the poorest user experience compared to the reactive and adaptive methods. The reactive method, while improving user experience over the proactive method, resulted in the poorest team performance. Our adaptive method maintained this improved user experience while offering an improved team performance compared to the reactive method. Our findings offer insights into the tradeoffs involved in the use of these methods and inform the future design of handover interactions for robots.},
author = {Huang, Chien-Ming and Cakmak, Maya and Mutlu, Bilge},
doi = {10.15607/RSS.2015.XI.031},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Cakmak, Mutlu - 2015 - Adaptive Coordination Strategies for Human-Robot Handovers.pdf:pdf},
isbn = {9780992374716},
issn = {2330765X},
journal = {Robotics: Science and Systems XI},
title = {{Adaptive Coordination Strategies for Human-Robot Handovers}},
url = {http://www.roboticsproceedings.org/rss11/p31.pdf},
year = {2015}
}
@article{Gharbi2015,
abstract = {Handing-over objects to humans (or taking objects from them) is a key capability for a service robot. Humans are efficient and natural while performing this action and the purpose of the studies on this topic is to bring human-robot handovers to an acceptable, efficient and natural level. This paper deals with the cues that allow to make a handover look as natural as possible, and more precisely we focus on where the robot should look while performing it. In this context we propose a user study, involving 33 volunteers, who judged video sequences where they see either a human or a robot giving them an object. They were presented with different sequences where the agents (robot or human) have different gaze behaviours, and were asked to give their feeling about the sequence naturalness. In addition to this subjective measure, the volunteers were equipped with an eye tracker which enabled us to have more accurate objective measures. I.},
author = {Gharbi, Mamoun and Paubel, Pierre Vincent and Clodic, Aurelie and Carreras, Ophelie and Alami, Rachid and Cellier, Jean Marie},
doi = {10.1109/ROMAN.2015.7333626},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gharbi et al. - 2015 - Toward a better understanding of the communication cues involved in a human-robot object transfer.pdf:pdf},
isbn = {9781467367042},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {319--324},
title = {{Toward a better understanding of the communication cues involved in a human-robot object transfer}},
volume = {2015-Novem},
year = {2015}
}
@article{Huebner2008a,
abstract = {Grasping is a central issue of various robot applications, especially when unknown objects have to be manipulated by the system. In earlier work, we have shown the efficiency of 3D object shape approximation by box primitives for the purpose of grasping. A point cloud was approximated by box primitives [1]. In this paper, we present a continuation of these ideas and focus on the box representation itself. On the number of grasp hypotheses from box face normals, we apply heuristic selection integrating task, orientation and shape issues. Finally, an off-line trained neural network is applied to chose a final best hypothesis as the final grasp. We motivate how boxes as one of the simplest representations can be applied in a more sophisticated manner to generate task-dependent grasps.},
author = {Huebner, Kai and Kragic, Danica},
doi = {10.1109/IROS.2008.4650722},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huebner, Kragic - 2008 - Selection of robot pre-grasps using box-based shape approximation.pdf:pdf},
isbn = {9781424420582},
issn = {2153-0858},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Dexterous Manipulation,Grasping},
pages = {1765--1770},
pmid = {4650722},
title = {{Selection of robot pre-grasps using box-based shape approximation}},
year = {2008}
}
@article{Prada2013,
abstract = {This article presents the current state of an ongoing work on Human-Robot interaction in which two partners collaborate during an object hand-over interaction. The manipulator control is based on the Dynamic Movement Primitives model, specialized for the object hand-over context. The proposed modifications enable finer control of the dynamic of the DMP to align it to human control strategies, where the contributions of the feedforward and feedback parts of the control are different to the original DMP formulation. Furthermore, the proposed scheme handles moving goals. With these two modifications, the model no longer requires an explicit estimation of the exchange position and it can generate motion purely reactively to the instantaneous position of the human hand. The quality of the control system is evaluated through an extensive comparison with ground truth data related to the object interaction between two humans acquired in the context of the European project CogLaboration which envisages an application in an industrial setting.},
author = {Prada, Miguel and Remazeilles, Anthony and Koene, Ansgar and Endo, Satoshi},
doi = {10.1109/IROS.2013.6696498},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prada et al. - 2013 - Dynamic Movement Primitives for Human-Robot interaction Comparison with human behavioral observation.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1168--1175},
title = {{Dynamic Movement Primitives for Human-Robot interaction: Comparison with human behavioral observation}},
year = {2013}
}
@article{Aleotti2014,
abstract = {A method for robot to human object hand-over is presented that takes into account user comfort. Comfort is addressed by serving the object to facilitate user's convenience. The object is delivered so that the most appropriate part is oriented towards the person interacting with the robot. This approach, aimed at contributing to the development of socially aware robots, has not been considered in previous works. The robot system also supports sensory-motor skills like object and people detection, robot grasping and motion planning. The experimental setup consists of a six degrees of freedom robot arm with both an eye-in-hand laser scanner and a fixed range sensor. The user interacting with the robot can assume an arbitrary position in front of the robot. Experiments are reported from a user study.},
author = {Aleotti, Jacopo and Micelli, Vincenzo and Caselli, Stefano},
doi = {10.1007/s12369-014-0241-3},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aleotti, Micelli, Caselli - 2014 - An Affordance Sensitive System for Robot to Human Object Handover.pdf:pdf},
isbn = {9781467346054},
issn = {18754805},
journal = {International Journal of Social Robotics},
keywords = {Grasp affordances,Human-robot interaction,Object handover},
number = {4},
pages = {653--666},
title = {{An Affordance Sensitive System for Robot to Human Object Handover}},
volume = {6},
year = {2014}
}
@article{Kang1993,
abstract = {Many of the tasks that are potential candidates for automation involve grasping. The authors are interested in the programming of robots to perform grasping tasks. To do this, the assembly plan from observation (APO) paradigm is adopted, where the key idea is to enable a system to observe a human performing a grasping task, understand it, and perform the task with minimal human intervention. A grasping task is composed of three phases: pregrasp phase, static grasp phase, and manipulation phase. The first step in recognizing a grasping task is identifying the grasp itself (within the static grasp phase). The proposed strategy of identifying the grasp is to map the low-level hand configuration to increasingly more abstract grasp descriptions. The abstract grasp descriptions are useful because they are manipulator-independent. To achieve the mapping, a grasp representation is introduced that is called the contact web, which is composed of a pattern of effective contact points between the hand and the object. A grasp taxonomy based on the contact web is also proposed as a tool to systematically identify a grasp. The grasp can be described at higher conceptual levels using a certain mapping function that results in an index called the grasp cohesive index. This index can be used to identify the grasp. Results from grasping experiments show that it is possible to distinguish between various types of grasps using the proposed contact web, grasp taxonomy and grasp cohesive index.},
author = {Kang, Sing Bing and Ikeuchi, Katsushi},
doi = {10.1109/70.246054},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kang, Ikeuchi - 1993 - Toward Automatic Robot Instruction from Perception—Recognizing a Grasp from Observation.pdf:pdf},
isbn = {1042-296X},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {4},
pages = {432--443},
title = {{Toward Automatic Robot Instruction from Perception—Recognizing a Grasp from Observation}},
volume = {9},
year = {1993}
}
@article{Cutkosky1990,
abstract = {In studying grasping and manipulation we find two very diff- erent approaches to the subject: knowledge-based approaches based pri- marily on empirical studies of human grasping and manipulation, and ana- lytical approaches based primarily on physical models of the manipulation process. This chapter begins with a review of studies of human grasping, in particular our development of a grasp taxonomy and an expert system for predicting human grasp choice. These studies show how object geometry and task requirements (as well as hand capabilities and tactile sensing) com- bine to dictate grasp choice. We then consider analytic models of grasping and manipulation with robotic hands. To keep the mathematics tractable, these models require numerous simplifications which restrict their general- ity. Despite their differences, the two approaches can be correlated. This provides insight into why people grasp and manipulate objects as they do, and suggests different approaches for robotic grasp and manipulation plan- ning. The results also bear upon such issues such as object representation and hand design.},
author = {Cutkosky, Mark R. and Howe, Robert D.},
doi = {10.1007/978-1-4613-8974-3_1},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cutkosky, Howe - 1990 - Human Grasp Choice and Robotic Grasp Analysis.pdf:pdf},
isbn = {9781461389767},
journal = {Dextrous Robot Hands},
pages = {5--31},
title = {{Human Grasp Choice and Robotic Grasp Analysis}},
url = {http://link.springer.com/10.1007/978-1-4613-8974-3{\_}1},
year = {1990}
}
@article{Huebner2008,
abstract = {Thinking about intelligent robots involves consideration of how such systems can be enabled to perceive, interpret and act in arbitrary and dynamic environments. While sensor perception and model interpretation focus on the robot's internal representation of the world rather passively, robot grasping capabilities are needed to actively execute tasks, modify scenarios and thereby reach versatile goals. These capabilities should also include the generation of stable grasps to safely handle even objects unknown to the robot. We believe that the key to this ability is not to select a good grasp depending on the identification of an object (e.g. as a cup), but on its shape (e.g. as a composition of shape primitives). In this paper, we envelop given 3D data points into primitive box shapes by a fit-and-split algorithm that is based on an efficient Minimum Volume Bounding Box implementation. Though box shapes are not able to approximate arbitrary data in a precise manner, they give efficient clues for planning grasps on arbitrary objects. We present the algorithm and experiments using the 3D grasping simulator Grasplt!.},
author = {Huebner, Kai and Ruthotto, Steffen and Kragic, Danica},
doi = {10.1109/ROBOT.2008.4543434},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huebner, Ruthotto, Kragic - 2008 - Minimum volume bounding box decomposition for shape approximation in robot grasping.pdf:pdf},
isbn = {9781424416479},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Dexterous Manipulation,Grasping,Manipulation Planning},
pages = {1628--1633},
title = {{Minimum volume bounding box decomposition for shape approximation in robot grasping}},
year = {2008}
}
@article{Pandey2012,
abstract = {In a typical Human-Robot Interaction (HRI) scenario, the robot needs to perform various tasks for the human, hence should take into account human oriented constraints. In this context it is not sufficient that the robot selects grasp and placement of the object from the stability point of view only. Motivated from human behavioral psychology, in this paper we emphasize on the mutually depended nature of grasp and placement selections, which is further constrained by the task, the environment and the human's perspective. We will explore essential human oriented constraints on grasp and placement selections and present a framework to incorporate them in synthesizing key configurations of planning basic interactive manipulation tasks.},
author = {Pandey, Amit Kumar and Saut, Jean Philippe and Sidobre, Daniel and Alami, Rachid},
doi = {10.1109/BioRob.2012.6290776},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pandey et al. - 2012 - Towards planning Human-Robot Interactive manipulation tasks Task dependent and human oriented autonomous selectio.pdf:pdf},
isbn = {9781457711992},
issn = {21551774},
journal = {Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics},
pages = {1371--1376},
title = {{Towards planning Human-Robot Interactive manipulation tasks: Task dependent and human oriented autonomous selection of grasp and placement}},
year = {2012}
}
@article{Chan2013,
abstract = {Object handover is a basic task that is found in many human-robot cooperation scenarios. If we are to build socially acceptable robots, we need to enable robots to perform handovers properly. In this paper, we discuss some of the social implications of proper robot-human handovers, and we focus on the challenge of determining a proper grasp configuration when handing over an object. We propose a framework that enables a robot to learn proper grasp configurations for handovers through observations. Our aim is to eliminate the need for manually specifying grasp configurations information to the robot, and allow generalization of handover grasp configurations for known objects to unknown objects. We are currently implementing our proposed framework onto an HRP4R robot, and we discuss about our plans for conducting user studies to evaluate our system upon its completion.},
author = {Chan, Wesley P. and Kumagai, Iori and Nozawa, Shunichi and Kakiuchi, Yohei and Okada, Kei and Inaba, Masayuki},
doi = {10.1109/ARSO.2013.6705512},
file = {:home/ximon/school/thesis/doc/doc/ref/06705512.pdf:pdf},
isbn = {9781479923694},
issn = {21627568},
journal = {Proceedings of IEEE Workshop on Advanced Robotics and its Social Impacts, ARSO},
pages = {94--99},
title = {{Creating socially acceptable robots: Leaning grasp configurations for object handovers from demonstrations}},
year = {2013}
}
@article{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Ding2004,
abstract = {Principal component analysis (PCA) is a widely used statistical technique for unsupervised$\backslash$tdimension$\backslash$treduction.$\backslash$tK -means$\backslash$tclustering is a commonly used data clustering for performing unsupervised learning tasks. Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering. New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix. These results indicate that unsupervised dimension reduction is closely related to unsupervised learning. Several implications are discussed. On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noisereduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data. On learning, the result suggests effective$\backslash$ttechniques$\backslash$tfor$\backslash$tK -means$\backslash$tdata$\backslash$tclustering. DNA gene expression and Internet newsgroups are analyzed to illustrate our results. Experiments indicate that the new bounds are within 0.5-1.5{\%} of the optimal values.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Ding, C and He, X},
doi = {10.1145/1015330.1015408},
eprint = {arXiv:0711.0189v1},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, He - 2004 - K-means clustering via principal component analysis(2).pdf:pdf},
isbn = {1581138385},
issn = {1581138385},
journal = {Proceedings of the twenty-first international conference on Machine learning},
keywords = {{\&}account {\&}activate {\&}adjacency {\&}balanced {\&}blind {\&}bo},
number = {2000},
pages = {29},
pmid = {19784854},
title = {{K-means clustering via principal component analysis}},
url = {papers://b6c7d293-c492-48a4-91d5-8fae456be1fa/Paper/p2738{\%}5Cnfile:///C:/Users/Serguei/OneDrive/Documents/Papers/K-means clustering via principal component.pdf},
volume = {Cl},
year = {2004}
}
@article{Morales,
author = {Morales, A and Chinellato, E and Sanz, P J and Fagg, A H and Pobil, A P},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morales et al. - Unknown - Vision based planar grasp synthesis and reliability assessment for a multifinger robot hand a learning appro.pdf:pdf},
title = {{Vision based planar grasp synthesis and reliability assessment for a multifinger robot hand : a learning approach}}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Arge2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Arge, F O R L and Mage, Cale I},
eprint = {arXiv:1409.1556v6},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arge, Mage - 2015 - V d c n l -s i r.pdf:pdf},
pages = {1--14},
title = {{V d c n l -s i r}},
year = {2015}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Radford, Metz, Chintala - 2015 - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Chan2015a,
abstract = {As humanoids work alongside people, there will be many situations where they need to handover objects to people. If humanoids are to fulfill their purpose effectively, it is imperative that they perform handovers properly. When handing over an object, the giver needs to determine where to grasp and how to orient the object properly in order to ensure a safe and efficient handover. We propose and implement a framework for automatically learning handover grasp points and orientations - which we refer to collectively as the handover grasp configuration - by observing how people hand over the objects to the robot. We achieve this using a skeleton tracker and a particle filter based object tracker. Our system requires no additional external cameras, or any markers on the person or the object. As far as we know, this is the first system that offers such capabilities for learning handover grasp configurations. An implementation on an HRP2V robot and an experiment with three different objects verified that our framework is capable of extracting and learning grasp configurations from handover demonstrations, and subsequently using the learned grasp configurations to handover the objects.},
author = {Chan, Wesley P. and Nagahama, Kotaro and Yaguchi, Hiroaki and Kakiuchi, Yohei and Okada, Kei and Inaba, Masayuki},
doi = {10.1109/HUMANOIDS.2015.7363492},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan et al. - 2015 - Implementation of a framework for learning handover grasp configurations through observation during human-robot obj.pdf:pdf},
isbn = {9781479968855},
issn = {21640580},
journal = {IEEE-RAS International Conference on Humanoid Robots},
keywords = {Handover,Receivers,Robot kinematics,Three-dimensional displays,Tracking},
pages = {1115--1120},
title = {{Implementation of a framework for learning handover grasp configurations through observation during human-robot object handovers}},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Jarrett2009,
abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63{\%} recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6{\%}) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({\&}gt; 65{\%}), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53{\%}).},
archivePrefix = {arXiv},
arxivId = {1602.05473},
author = {Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459469},
eprint = {1602.05473},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jarrett et al. - 2009 - What is the best multi-stage architecture for object recognition.pdf:pdf},
isbn = {9781424444205},
issn = {1550-5499},
pmid = {25719670},
title = {{What is the best multi-stage architecture for object recognition?}},
year = {2009}
}
@inproceedings{Lee2009,
abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3605v3},
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553453},
eprint = {arXiv:1301.3605v3},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2009 - Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.pdf:pdf},
isbn = {9781605585161},
issn = {02643294},
pmid = {20957573},
title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
year = {2009}
}
@article{Turaga2010,
abstract = {Many image segmentation algorithms first generate an affinity graph and then partition it. We present a machine learning approach to computing an affinity graph using a convolutional network (CN) trained using ground truth provided by human experts. The CN affinity graph can be paired with any standard partitioning algorithm and improves segmentation accuracy significantly compared to standard hand-designed affinity functions. We apply our algorithm to the challenging 3D segmentation problem of reconstructing neuronal processes from volumetric electron microscopy (EM) and show that we are able to learn a good affinity graph directly from the raw EM images. Further, we show that our affinity graph improves the segmentation accuracy of both simple and sophisticated graph partitioning algorithms. In contrast to previous work, we do not rely on prior knowledge in the form of hand-designed image features or image preprocessing. Thus, we expect our algorithm to generalize effectively to arbitrary image types.},
author = {Turaga, Srinivas C. and Murray, Joseph F. and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and {Sebastian Seung}, H.},
doi = {10.1162/neco.2009.10-08-881},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Turaga et al. - 2010 - Convolutional networks can learn to generate affinity graphs for image segmentation.pdf:pdf},
isbn = {1530-888X (Electronic)$\backslash$r0899-7667 (Linking)},
issn = {08997667},
journal = {Neural Computation},
number = {2},
pages = {511--538},
pmid = {19922289},
title = {{Convolutional networks can learn to generate affinity graphs for image segmentation}},
volume = {22},
year = {2010}
}
@article{Saxena2008,
abstract = {Abstract We consider the problem of grasping novel objects , specifically objects that are being seen for the first time through vision . Grasping a previously unknown object, one for which a 3-d model is not available, is a challenging problem. Furthermore, even if given a ... $\backslash$n},
author = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew Y.},
doi = {10.1177/0278364907087172},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saxena, Driemeyer, Ng - 2008 - Robotic grasping of novel objects using vision.pdf:pdf},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Grasping,Learning and adaptive systems,Perception,Personal robots,Robotics,Vision of grasping},
number = {2},
pages = {157--173},
pmid = {253318100002},
title = {{Robotic grasping of novel objects using vision}},
volume = {27},
year = {2008}
}
@article{Redmon2014,
abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
archivePrefix = {arXiv},
arxivId = {1412.3128},
author = {Redmon, Joseph and Angelova, Anelia},
eprint = {1412.3128},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Angelova - 2014 - Real-Time Grasp Detection Using Convolutional Neural Networks.pdf:pdf},
isbn = {9781479969227},
title = {{Real-Time Grasp Detection Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1412.3128},
year = {2014}
}
@article{Feix2009,
abstract = {The goal of this work is to overview and summarize the grasping taxonomies reported in the literature. Our long term goal is to understand how to reduce mechanical complexity of anthropomorphic hands and still preserve their dexterity. On the basis of a literature survey, 33 different grasp types are taken into account. They were then arranged in a hierarchical manner, resulting in 17 grasp types},
author = {Feix, Thomas and Pawlik, Roland and Schmiedmayer, Heinz-Bodo and Romero, Javier and Kragi, Danica},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Feix et al. - 2009 - A comprehensive grasp taxonomy.pdf:pdf},
journal = {Robotics, Science and Systems Conference: Workshop on Understanding the Human Hand for Advancing Robotic Manipulation},
pages = {2--3},
title = {{A comprehensive grasp taxonomy}},
year = {2009}
}
@article{Jain2013,
abstract = {To address the problem of estimating the effects of unknown tools, we propose a novel concept of tool representation based on the functional features of the tool. We argue that functional features remain distinctive and invariant across different tools used for performing similar tasks. Such a representation can be used to estimate the effects of unknown tools that share similar functional features. To learn the usages of tools to physically alter the environment, a robot should be able to reason about its capability to act, the representation of available tools, and effect of manipulating tools. To enable a robot to perform such reasoning, we present a novel approach, called Tool Affordances, to learn bi-directional causal relationships between actions, functional features and the effects of tools. A Bayesian network is used to model tool affordances because of its capability to model probabilistic dependencies between data. To evaluate the learnt tool affordances, we conducted an inference test in which a robot inferred suitable functional features to realize certain effects (including novel effects) from the given action. The results show that the generalization of functional features enables the robot to estimate the effects of unknown tools that have similar functional features. We validate the accuracy of estimation by error analysis. {\textcopyright} 2013 ISAROB.},
author = {Jain, Raghvendra and Inamura, Tetsunari},
doi = {10.1007/s10015-013-0105-1},
file = {:home/ximon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain, Inamura - 2013 - Bayesian learning of tool affordances based on generalization of functional feature to estimate effects of unseen.pdf:pdf},
issn = {14335298},
journal = {Artificial Life and Robotics},
keywords = {Bayesian networks,Functional features of tools,Probabilistic modeling,Tool affordances,Tool manipulation},
number = {1-2},
pages = {95--103},
title = {{Bayesian learning of tool affordances based on generalization of functional feature to estimate effects of unseen tools}},
volume = {18},
year = {2013}
}
