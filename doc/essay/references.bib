@article{Suay2015,
abstract = {We present a novel approach for generating candidate handover positions for human receivers. The object handover problem involves several variables and is under-constrained. The sheer number of possibilities makes it nontrivial to find a good object handover position for existing algorithms. To address this problem, we introduce the use of a custom biomechanical model in a novel algorithm for generating a handover position. The biomechanical model serves as a tool for approximating the static strength a human receiver needs to exert in order to receive an object. A mobile manipulator can use the output of our algorithm when calculating a target position for handing over an object to a human. We show preliminary results involving electromyography data and joint moment values, as well as a proof-of-concept implementation with a mobile manipulator to fetch a toolbox to a receiver.},
author = {Suay, Halit Bener and Sisbot, Emrah Akin},
doi = {10.1109/ICRA.2015.7139724},
file = {:home/ximon/school/thesis/doc/ref/a-position-generation-algorithm-utilizing-a-biomechanical-model-for-robot-human-object-handover.pdf:pdf},
isbn = {VO  -},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
number = {June},
pages = {3776--3781},
title = {{A position generation algorithm utilizing a biomechanical model for robot-human object handover}},
volume = {2015-June},
year = {2015}
}
@article{Huang2015,
abstract = {Handovers of objects are critical interactions that frequently arise in physical collaborations. In such interactions, humans naturally monitor the pace and workload of their partners and adapt their handovers accordingly. In this paper, we investigate how robots designed to engage in physical collaborations may achieve similar adaptivity in performing handovers. To that end, we collected and analyzed data from human dyads performing a common household task unloading a dish rack where receivers had different levels of task demands. We identified two coordination strategies that enabled givers to adapt to receivers task demands. We then formulated and implemented these strategies on a robotic manipulator. The implemented autonomous system was evaluated in a human-robot interaction study against two baselines that use proactive and reactive coordination methods. The results show a tradeoff between team performance and user experience when human receivers had greater task demands. In particular, the proactive method provided the greatest levels of team performance but offered the poorest user experience compared to the reactive and adaptive methods. The reactive method, while improving user experience over the proactive method, resulted in the poorest team performance. Our adaptive method maintained this improved user experience while offering an improved team performance compared to the reactive method. Our findings offer insights into the tradeoffs involved in the use of these methods and inform the future design of handover interactions for robots.},
author = {Huang, Chien-Ming and Cakmak, Maya and Mutlu, Bilge},
doi = {10.15607/RSS.2015.XI.031},
file = {:home/ximon/school/thesis/doc/ref/adaptive-coordination-strategies-for-human-robo-handovers.pdf:pdf},
isbn = {9780992374716},
issn = {2330765X},
journal = {Robotics: Science and Systems XI},
title = {{Adaptive Coordination Strategies for Human-Robot Handovers}},
url = {http://www.roboticsproceedings.org/rss11/p31.pdf},
year = {2015}
}
@article{Kim2004,
abstract = {For home-service robots one of the most common interactions with humans is the handing over of objects using arms and hands of anthropomorphic robots. We propose advanced methods of determination of grasp sites for handover operations that incorporate especially etiquettes which factors include object shapes, object functions, and safety. We also address different operations for handover: one-handed handover with one grasp, two-step handover with midair re-grasp, and two-handed handover. We show the effectiveness of our algorithm for interaction between humans and robots with graphical simulations.},
author = {Kim, Jinsul and Park, Jihwan and Hwang, Yong and Lee, Manjai},
file = {:home/ximon/school/thesis/doc/ref/advanced-grasp-planning-for-handover-operation-between-human-and-robot.pdf:pdf},
journal = {2nd International Conference on Autonomous Robots and Agents},
keywords = {dual-arms-and-hands,etiquette,grasp-planning,handover-operation,humanoid-home-service-robot,motion-planning},
number = {c},
pages = {34--39},
title = {{Advanced Grasp Planning for Handover Operation Between Human and Robot: Three Handover Methods in Esteem Etiquettes Using Dual Arms and Hands of Home-Service Robot}},
url = {citeulike-article-id:6221504},
year = {2004}
}
@article{Aleotti2014,
abstract = {A method for robot to human object hand-over is presented that takes into account user comfort. Comfort is addressed by serving the object to facilitate user's convenience. The object is delivered so that the most appropriate part is oriented towards the person interacting with the robot. This approach, aimed at contributing to the development of socially aware robots, has not been considered in previous works. The robot system also supports sensory-motor skills like object and people detection, robot grasping and motion planning. The experimental setup consists of a six degrees of freedom robot arm with both an eye-in-hand laser scanner and a fixed range sensor. The user interacting with the robot can assume an arbitrary position in front of the robot. Experiments are reported from a user study.},
author = {Aleotti, Jacopo and Micelli, Vincenzo and Caselli, Stefano},
doi = {10.1007/s12369-014-0241-3},
file = {:home/ximon/school/thesis/doc/ref/an-affordance-sensitive-system-for-robot-to-human-object-handover.pdf:pdf},
isbn = {9781467346054},
issn = {18754805},
journal = {International Journal of Social Robotics},
keywords = {Grasp affordances,Human-robot interaction,Object handover},
number = {4},
pages = {653--666},
title = {{An Affordance Sensitive System for Robot to Human Object Handover}},
volume = {6},
year = {2014}
}
@article{Miller2003,
author = {Miller, Andrew T and Knoop, Steffen and Christensen, Henrik I and Allen, Peter K},
file = {:home/ximon/school/thesis/doc/ref/automatic-grasp-planning-using-shape-primitives.pdf:pdf},
isbn = {0780377362},
journal = {Proceedings of IEEE International Conference on Robotics and Automation},
pages = {1824--1829},
title = {{Automatic Grasp Planning Using Shape Primitives}},
year = {2003}
}
@article{Jain2013,
abstract = {To address the problem of estimating the effects of unknown tools, we propose a novel concept of tool representation based on the functional features of the tool. We argue that functional features remain distinctive and invariant across different tools used for performing similar tasks. Such a representation can be used to estimate the effects of unknown tools that share similar functional features. To learn the usages of tools to physically alter the environment, a robot should be able to reason about its capability to act, the representation of available tools, and effect of manipulating tools. To enable a robot to perform such reasoning, we present a novel approach, called Tool Affordances, to learn bi-directional causal relationships between actions, functional features and the effects of tools. A Bayesian network is used to model tool affordances because of its capability to model probabilistic dependencies between data. To evaluate the learnt tool affordances, we conducted an inference test in which a robot inferred suitable functional features to realize certain effects (including novel effects) from the given action. The results show that the generalization of functional features enables the robot to estimate the effects of unknown tools that have similar functional features. We validate the accuracy of estimation by error analysis. {\textcopyright} 2013 ISAROB.},
author = {Jain, Raghvendra and Inamura, Tetsunari},
doi = {10.1007/s10015-013-0105-1},
file = {:home/ximon/school/thesis/doc/ref/bayesian-learning-of-tool-affordance.pdf:pdf},
issn = {14335298},
journal = {Artificial Life and Robotics},
keywords = {Bayesian networks,Functional features of tools,Probabilistic modeling,Tool affordances,Tool manipulation},
number = {1-2},
pages = {95--103},
title = {{Bayesian learning of tool affordances based on generalization of functional feature to estimate effects of unseen tools}},
volume = {18},
year = {2013}
}
@article{Chan2015,
abstract = {To enable robots to learn handover orientations from observing natural handovers, we conduct a user study to measure and compare natural handover orientations with giver-centered and receiver-centered handover orientations for twenty common objects. We use a distance minimization approach to compute mean handover orientations. We posit that, computed means of receiver-centered orientations could be used by robot givers to achieve more efficient and socially acceptable handovers. Furthermore, we introduce the notion of affordance axes for comparing handover orientations, and offer a definition for computing them. Observable patterns were found in receiver-centered handover orientations. Comparisons show that depending on the object, natural handover orientations may not be receiver-centered; thus, robots may need to distinguish between good and bad handover orientations when learning from natural handovers.},
author = {Chan, Wesley P. and Pan, Matthew K.X.J. and Croft, Elizabeth A. and Inaba, Masayuki},
doi = {10.1109/IROS.2015.7353106},
file = {:home/ximon/school/thesis/doc/ref/characterization-of-handover-orientations-used-by-humans-for-efficient-robot-to-human-handovers.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Atmospheric measurements,Collaboration,Handover,Particle measurements,Receivers,Robots},
pages = {1--6},
title = {{Characterization of handover orientations used by humans for efficient robot to human handovers}},
volume = {2015-December},
year = {2015}
}
@article{Aleotti2012,
abstract = {A method for robot to human object hand-over is presented that takes into account user comfort. Comfort is addressed by serving the object to facilitate user's convenience. The object is delivered so that the most appropriate part is oriented towards the person interacting with the robot. This approach, aimed at contributing to the development of socially aware robots, has not been considered in previous works. The robot system also supports sensory-motor skills like object and people detection, robot grasping and motion planning. The experimental setup consists of a six degrees of freedom robot arm with both an eye-in-hand laser scanner and a fixed range sensor. The user interacting with the robot can assume an arbitrary position in front of the robot. Experiments are reported from a user study.},
author = {Aleotti, Jacopo and Micelli, Vincenzo and Caselli, Stefano},
doi = {10.1109/ROMAN.2012.6343845},
file = {:home/ximon/school/thesis/doc/ref/comfortable-robot-human-object-hand-over.pdf:pdf},
isbn = {9781467346054},
issn = {18754805},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {771--776},
title = {{Comfortable robot to human object hand-over}},
year = {2012}
}
@article{Lenz2015,
abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.},
archivePrefix = {arXiv},
arxivId = {1301.3592},
author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
doi = {10.1177/0278364914549607},
eprint = {1301.3592},
file = {:home/ximon/school/thesis/doc/ref/deep-learning-for-detecting-robotic-grasps.pdf:pdf},
isbn = {9789810739379},
issn = {17413176},
journal = {International Journal of Robotics Research},
keywords = {3D feature learning,Baxter,PR2,RGB-D multi-modal data,Robotic grasping,deep learning},
number = {4-5},
pages = {705--724},
pmid = {19352402},
title = {{Deep learning for detecting robotic grasps}},
volume = {34},
year = {2015}
}
@article{Admoni2014,
abstract = {As assistive robots become popular in factories and homes, there is greater need for natural, multi-channel communication during collaborative manipulation tasks. Non-verbal communication such as eye gaze can provide information without overloading more taxing channels like speech. However, certain collaborative tasks may draw attention away from these subtle communication modalities. For instance, robot-to-human handovers are primarily manual tasks, and human attention is therefore drawn to robot hands rather than to robot faces during handovers. In this paper, we show that a simple manipulation of a robot's handover behavior can significantly increase both awareness of the robot's eye gaze and compliance with that gaze. When eye gaze communication occurs during the robot's release of an object, delaying object release until the gaze is finished draws attention back to the robot's head, which increases conscious perception of the robot's communication. Furthermore, the handover delay increases peoples' compliance with the robot's communication over a non-delayed handover, even when compliance results in counterintuitive behavior.},
author = {Admoni, Henny and Dragan, Anca and Srinivasa, Siddhartha S. and Scassellati, Brian},
doi = {10.1145/2559636.2559682},
file = {:home/ximon/school/thesis/doc/ref/deliberate-delays-during-robot-to-human-handovers-improve-compliance-with-gaze-communication.pdf:pdf},
isbn = {9781450326582},
issn = {21672148},
journal = {Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction - HRI '14},
pages = {49--56},
title = {{Deliberate delays during robot-to-human handovers improve compliance with gaze communication}},
url = {http://dl.acm.org/citation.cfm?doid=2559636.2559682},
year = {2014}
}
@article{Chan2014,
abstract = {We present a method for enabling robots to determine appropriate grasp configurations for handovers - i.e., where to grasp, and how to orient an object when handing it over. In our method, a robot first builds a knowledge base by observing demonstrations of how certain objects are used and their proper handover grasp configurations. Objects in the knowledge base are then organized based on their movements and inter-object interaction features. The key point in this process is that similarity in affordances should be recognized. When subsequently asked to handover an object, the robot then computes an appropriate grasp configuration based on the object's recognized affordances. Experimental results show that our method was able to differentiate and group together objects according to their affordances. Furthermore, when given a new object, our method was able to generalize data in the knowledge base and determine an appropriate grasp configuration. {\&}copy; 2014 IEEE.},
author = {Chan, Wesley P. and Kakiuchi, Yohei and Okada, Kei and Inaba, Masayuki},
doi = {10.1109/IROS.2014.6942733},
file = {:home/ximon/school/thesis/doc/ref/determining-proper-grasp-configurations-for-handovers-through-observations-of-object-movement-patterns-and-inter-object-interactions-during-usage.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Iros},
pages = {1355--1360},
title = {{Determining proper grasp configurations for handovers through observation of object movement patterns and inter-object interactions during usage}},
volume = {8656},
year = {2014}
}
@article{Prada2013,
abstract = {This article presents the current state of an ongoing work on Human-Robot interaction in which two partners collaborate during an object hand-over interaction. The manipulator control is based on the Dynamic Movement Primitives model, specialized for the object hand-over context. The proposed modifications enable finer control of the dynamic of the DMP to align it to human control strategies, where the contributions of the feedforward and feedback parts of the control are different to the original DMP formulation. Furthermore, the proposed scheme handles moving goals. With these two modifications, the model no longer requires an explicit estimation of the exchange position and it can generate motion purely reactively to the instantaneous position of the human hand. The quality of the control system is evaluated through an extensive comparison with ground truth data related to the object interaction between two humans acquired in the context of the European project CogLaboration which envisages an application in an industrial setting.},
author = {Prada, Miguel and Remazeilles, Anthony and Koene, Ansgar and Endo, Satoshi},
doi = {10.1109/IROS.2013.6696498},
file = {:home/ximon/school/thesis/doc/ref/dynamic-movement-primitives-for-human-robo-interaction{\_}comparison-with-human-behavioral-observation.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1168--1175},
title = {{Dynamic Movement Primitives for Human-Robot interaction: Comparison with human behavioral observation}},
year = {2013}
}
@article{Schettino2003,
abstract = {Normal subjects gradually preshape their hands during a grasping movement in order to conform the hand to the shape of a target object. The evolution of hand preshaping may depend on visual feedback about arm and hand position as well as on target shape and location at specific times during the movement. The present study manipulated object shape in order to produce differentiable patterns of finger placement along two orthogonal "dimensions" (flexion/extension and abduction/adduction), and manipulated the amount of available visual information during a grasp. Normal subjects were asked to reach to and grasp a set of objects presented in a randomized fashion at a fixed spatial location in three visual feedback conditions: Full Vision (both hand and target visible), Object Vision (only the object was visible but not the hand) and No Vision (vision of neither the hand nor the object during the movement). Flexion/extension angles of the metacarpophalangeal and proximal interphalangeal joints of the index, ring, middle and pinkie fingers as well as the abduction/adduction angles between the index-middle and middle-ring fingers were recorded. Kinematic analysis revealed that as visual feedback was reduced, movement duration increased and time to peak aperture of the hand decreased, in accord with previously reported studies. Analysis of the patterns of joint flexion/extension and abduction/adduction per object shape revealed that preshaping based on the abduction/adduction dimension occurred early during the reach for all visual feedback conditions (approximately 45{\%} of normalized movement time). This early preshaping across visual feedback conditions suggests the existence of mechanisms involved in the selection of basic hand configurations. Furthermore, while configuration changes in the flexion/extension dimension resulting in well-defined hand configurations occurred earlier during the movement in the Object Vision and No Vision conditions (45{\%}), those in the Full Vision condition were observed only after 75{\%} of the movement, as the moving hand entered the central region of the visual field. The data indicate that there are at least two control mechanisms at work during hand preshaping, an early predictive phase during which grip selection is attained regardless of availability of visual feedback and a late responsive phase during which subjects may use visual feedback to optimize their grasp.},
author = {Schettino, Luis F. and Adamovich, Sergei V. and Poizner, Howard},
doi = {10.1007/s00221-003-1435-3},
file = {:home/ximon/school/thesis/doc/ref/effects-of-object-shape-and-visual-feedback-on-hand-configuration-during-grasping.pdf:pdf},
isbn = {0014-4819 (Print)},
issn = {00144819},
journal = {Experimental Brain Research},
keywords = {Hand preshaping,Prehension,Visual feedback,Visuomotor control},
number = {2},
pages = {158--166},
pmid = {12783144},
title = {{Effects of object shape and visual feedback on hand configuration during grasping}},
volume = {151},
year = {2003}
}
@article{Jiang2011,
abstract = {Given an image and an aligned depth map of an object, our goal is to estimate the full 7-dimensional gripper configuration{\&}{\#}x2014;its 3D location, 3D orientation and the gripper opening width. Recently, learning algorithms have been successfully applied to grasp novel objects{\&}{\#}x2014;ones not seen by the robot before. While these approaches use low-dimensional representations such as a {\&}{\#}x2018;grasping point{\&}{\#}x2019; or a {\&}{\#}x2018;pair of points{\&}{\#}x2019; that are perhaps easier to learn, they only partly represent the gripper configuration and hence are sub-optimal. We propose to learn a new {\&}{\#}x2018;grasping rectangle{\&}{\#}x2019; representation: an oriented rectangle in the image plane. It takes into account the location, the orientation as well as the gripper opening width. However, inference with such a representation is computationally expensive. In this work, we present a two step process in which the first step prunes the search space efficiently using certain features that are fast to compute. For the remaining few cases, the second step uses advanced features to accurately select a good grasp. In our extensive experiments, we show that our robot successfully uses our algorithm to pick up a variety of novel objects.},
author = {Jiang, Yun and Moseson, Stephen and Saxena, Ashutosh},
doi = {10.1109/ICRA.2011.5980145},
file = {:home/ximon/school/thesis/doc/ref/jiang{\_}rectanglerepresentation{\_}fastgrasping.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3304--3311},
pmid = {5980145},
title = {{Efficient grasping from RGBD images: Learning using a new rectangle representation}},
year = {2011}
}
@article{Yang,
author = {Yang, Yezhou and Ferm, Cornelia},
file = {:home/ximon/school/thesis/doc/ref/grasp-type-revisited{\_}a-modern-perspective-on-classical-feature-for-vision.pdf:pdf},
number = {c},
title = {{Grasp Type Revisited : A Modern Perspective on A Classical Feature for Vision}}
}
@article{Chan2015a,
abstract = {As humanoids work alongside people, there will be many situations where they need to handover objects to people. If humanoids are to fulfill their purpose effectively, it is imperative that they perform handovers properly. When handing over an object, the giver needs to determine where to grasp and how to orient the object properly in order to ensure a safe and efficient handover. We propose and implement a framework for automatically learning handover grasp points and orientations - which we refer to collectively as the handover grasp configuration - by observing how people hand over the objects to the robot. We achieve this using a skeleton tracker and a particle filter based object tracker. Our system requires no additional external cameras, or any markers on the person or the object. As far as we know, this is the first system that offers such capabilities for learning handover grasp configurations. An implementation on an HRP2V robot and an experiment with three different objects verified that our framework is capable of extracting and learning grasp configurations from handover demonstrations, and subsequently using the learned grasp configurations to handover the objects.},
author = {Chan, Wesley P. and Nagahama, Kotaro and Yaguchi, Hiroaki and Kakiuchi, Yohei and Okada, Kei and Inaba, Masayuki},
doi = {10.1109/HUMANOIDS.2015.7363492},
file = {:home/ximon/school/thesis/doc/ref/implementation-framework-learning-handover-grasp-config-through-observations.pdf:pdf},
isbn = {9781479968855},
issn = {21640580},
journal = {IEEE-RAS International Conference on Humanoid Robots},
keywords = {Handover,Receivers,Robot kinematics,Three-dimensional displays,Tracking},
pages = {1115--1120},
title = {{Implementation of a framework for learning handover grasp configurations through observation during human-robot object handovers}},
volume = {2015-December},
year = {2015}
}
@article{Grigore2013,
abstract = {The development of trustworthy human-assistive robots is a challenge that goes beyond the traditional boundaries of engineering. Essential components of trustworthiness are safety, predictability and usefulness. In this paper we demonstrate that the integration of joint action understanding from human-human interaction into the human-robot context can significantly improve the success rate of robot-to-human object handover tasks. We take a two layer approach. The first layer handles the physical aspects of the handover. The robot's decision to release the object is informed by a Hidden Markov Model that estimates the state of the handover. Inspired by human-human handover observations, we then introduce a higher-level cognitive layer that models behaviour characteristic for a human user in a handover situation. In particular, we focus on the inclusion of eye gaze / head orientation into the robot's decision making. Our results demonstrate that by integrating these non-verbal cues the success rate of robot-to-human handovers can be significantly improved, resulting in a more robust and therefore safer system.},
author = {Grigore, Elena Corina and Eder, Kerstin and Pipe, Anthony G. and Melhuish, Chris and Leonards, Ute},
doi = {10.1109/IROS.2013.6697021},
file = {:home/ximon/school/thesis/doc/ref/join-action-understanding-improves-robot-to-human-object-handover.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4622--4629},
title = {{Joint action understanding improves robot-to-human object handover}},
year = {2013}
}
@article{Moon2014,
abstract = {In this paper we provide empirical evidence that using humanlike gaze cues during human-robot handovers can improve the timing and perceived quality of the handover event. Handovers serve as the foundation of many human-robot tasks. Fluent, legible handover interactions require appropriate nonverbal cues to signal handover intent, location and timing. Inspired by observations of human-human handovers, we implemented gaze behaviors on a PR2 humanoid robot. The robot handed over water bottles to a total of 102 na{\"{i}}ve subjects while varying its gaze behaviour: no gaze, gaze designed to elicit shared attention at the handover location, and the shared attention gaze complemented with a turn- taking cue. We compared subject perception of and reaction time to the robot-initiated handovers across the three gaze conditions. Results indicate that subjects reach for the offered object significantly earlier when a robot provides a shared attention gaze cue during a handover. We also observed a statistical trend of subjects preferring handovers with turn-taking gaze cues over the other conditions. Our work demonstrates that gaze can play a key role in improving user experience of human-robot handovers, and help make handovers fast and fluent.},
author = {Moon, AJung and Troniak, Daniel M. and Gleeson, Brian and Pan, Matthew K. X. J. and Zeng, Minhua and Blumer, Benjamin A. and MacLean, Karon and Croft, Elizabeth A.},
doi = {10.1145/2559636.2559656},
file = {:home/ximon/school/thesis/doc/ref/meet-me-where-im-gazing{\_}how-shared-attention-gaze-affects-human-robo-handover-timing.pdf:pdf},
isbn = {9781450326582},
issn = {21672148},
journal = {Human-Robot Interaction},
keywords = {gaze,handover,head gaze,human-robot communication,nonverbal communication,turn-taking},
pages = {334--341},
title = {{Meet me where i'm gazing: how shared attention gaze affects human-robot handover timing}},
url = {http://dl.acm.org/citation.cfm?id=2559636.2559656},
year = {2014}
}
@article{Huebner2008,
abstract = {Thinking about intelligent robots involves consideration of how such systems can be enabled to perceive, interpret and act in arbitrary and dynamic environments. While sensor perception and model interpretation focus on the robot's internal representation of the world rather passively, robot grasping capabilities are needed to actively execute tasks, modify scenarios and thereby reach versatile goals. These capabilities should also include the generation of stable grasps to safely handle even objects unknown to the robot. We believe that the key to this ability is not to select a good grasp depending on the identification of an object (e.g. as a cup), but on its shape (e.g. as a composition of shape primitives). In this paper, we envelop given 3D data points into primitive box shapes by a fit-and-split algorithm that is based on an efficient Minimum Volume Bounding Box implementation. Though box shapes are not able to approximate arbitrary data in a precise manner, they give efficient clues for planning grasps on arbitrary objects. We present the algorithm and experiments using the 3D grasping simulator Grasplt!.},
author = {Huebner, Kai and Ruthotto, Steffen and Kragic, Danica},
doi = {10.1109/ROBOT.2008.4543434},
file = {:home/ximon/school/thesis/doc/ref/to-read/minimum-volume-bounding-box-decomposition-for-shape-approximation-in-robot-grasping.pdf:pdf},
isbn = {9781424416479},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Dexterous Manipulation,Grasping,Manipulation Planning},
pages = {1628--1633},
title = {{Minimum volume bounding box decomposition for shape approximation in robot grasping}},
year = {2008}
}
@article{Redmon2014,
abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
archivePrefix = {arXiv},
arxivId = {1412.3128},
author = {Redmon, Joseph and Angelova, Anelia},
eprint = {1412.3128},
file = {:home/ximon/school/thesis/doc/ref/real-time-grasp-detection-using-cnn.pdf:pdf},
isbn = {9781479969227},
title = {{Real-Time Grasp Detection Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1412.3128},
year = {2014}
}
@article{Koene2014,
abstract = {In current society there is a growing call for robotic platforms designed to provide direct assistance to humans within the near future. An essential requirement, if robots are to become accepted as service providers that interact directly with humans, is that the human-robot interactions must not only be safe and reliable, but also produce a satisfying user experience. Here we report results from a study on human-robot object handovers in which we show that timing aspect of the robot response outweighed spatial precision for determining user experience ratings.},
author = {Koene, Ansgar and Remazeilles, Anthony and Prada, Miguel and Garzo, Ainara and Puerto, Mildred and Endo, Satoshi and Wing, Alan M},
file = {:home/ximon/school/thesis/doc/ref/relative-importance-of-spatial-temporal-precision-for-user-satisfaction-in-human-robo-object-handover-interactions.pdf:pdf},
journal = {Third International Symposium on New Frontiers in Human-Robot Interaction},
number = {October 2015},
title = {{Relative importance of spatial and temporal precision for user satisfaction in human-robot object handover interactions}},
year = {2014}
}
@article{Saxena2008,
abstract = {Abstract We consider the problem of grasping novel objects , specifically objects that are being seen for the first time through vision . Grasping a previously unknown object, one for which a 3-d model is not available, is a challenging problem. Furthermore, even if given a ... $\backslash$n},
author = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew Y.},
doi = {10.1177/0278364907087172},
file = {:home/ximon/school/thesis/doc/ref/robot-grasping-of-novel-objects-using-vision.pdf:pdf},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Grasping,Learning and adaptive systems,Perception,Personal robots,Robotics,Vision of grasping},
number = {2},
pages = {157--173},
pmid = {253318100002},
title = {{Robotic grasping of novel objects using vision}},
volume = {27},
year = {2008}
}
@article{Mainprice2012,
abstract = {For a versatile human-assisting mobile-manipulating robot such as the PR2, handing over objects to humans in possibly cluttered workspaces is a key capability. In this paper we investigate the motion planning of handovers while accounting for the human mobility. We treat the human motion as part of the planning problem thus enabling to find broader type of handing strategies. We formalize the problem and propose an algorithmic solution taking into account the HRI constraints induced by the human receiver presence. Simulation results with the PR2 robot illustrate the efficacy of the approach.},
author = {Mainprice, Jim and Gharbi, Mamoun and Simeon, Thierry and Alami, Rachid},
doi = {10.1109/ROMAN.2012.6343844},
file = {:home/ximon/school/thesis/doc/ref/sharing-effort-in-planning-human-robot-handover-tasks.pdf:pdf},
isbn = {9781467346054},
issn = {1944-9445},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {764--770},
title = {{Sharing effort in planning human-robot handover tasks}},
year = {2012}
}
@article{Sato2002,
abstract = { In order to transmit, share and store human knowledge, it is important for a robot to be able to acquire task models and skills by observing human actions. Since vision plays an important role for observation, we propose a technique for measuring the position and posture of objects and hands in 3-dimensional space at high speed and with high precision by vision. Next, we show a framework for the analysis and description of a task by using an object functions as elements.},
author = {Sato, Y. and Bernardin, K. and Kimura, H. and Ikeuchi, K.},
doi = {10.1109/IRDS.2002.1043898},
file = {:home/ximon/school/thesis/doc/ref/task-analysis-based-on-observing-hands-and-objects-by-vision.pdf:pdf},
isbn = {0-7803-7398-7},
journal = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
number = {October},
pages = {5--10},
title = {{Task analysis based on observing hands and objects by vision}},
volume = {2},
year = {2002}
}
@article{Song2015,
abstract = {Grasping and manipulating everyday objects in a goal-directed manner is an important ability of a service robot. The robot needs to reason about task requirements and ground these in the sensorimotor information. Grasping and interaction with objects are challenging in real-world scenarios, where sensorimotor uncertainty is prevalent. This paper presents a probabilistic framework for the representation and modeling of robot-grasping tasks. The framework consists of Gaussian mixture models for generic data discretization, and discrete Bayesian networks for encoding the probabilistic relations among various task-relevant variables, including object and action features as well as task constraints. We evaluate the framework using a grasp database generated in a simulated environment including a human and two robot hand models. The generative modeling approach allows the prediction of grasping tasks given uncertain sensory data, as well as object and grasp selection in a task-oriented manner. Furthermore, the graphical model framework provides insights into dependencies between variables and features relevant for object grasping.},
author = {Song, Dan and Ek, Carl Henrik and Huebner, Kai and Kragic, Danica},
doi = {10.1109/TRO.2015.2409912},
file = {:home/ximon/school/thesis/doc/ref/task-based-robot-grasp-planning-using-probabilistic-inference.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Cognitive human-robot interaction,grasping,learning and adaptive systems,probabilistic graphical models,recognition},
number = {3},
pages = {546--561},
title = {{Task-Based Robot Grasp Planning Using Probabilistic Inference}},
volume = {31},
year = {2015}
}
@article{Kang1993,
abstract = {Many of the tasks that are potential candidates for automation involve grasping. The authors are interested in the programming of robots to perform grasping tasks. To do this, the assembly plan from observation (APO) paradigm is adopted, where the key idea is to enable a system to observe a human performing a grasping task, understand it, and perform the task with minimal human intervention. A grasping task is composed of three phases: pregrasp phase, static grasp phase, and manipulation phase. The first step in recognizing a grasping task is identifying the grasp itself (within the static grasp phase). The proposed strategy of identifying the grasp is to map the low-level hand configuration to increasingly more abstract grasp descriptions. The abstract grasp descriptions are useful because they are manipulator-independent. To achieve the mapping, a grasp representation is introduced that is called the contact web, which is composed of a pattern of effective contact points between the hand and the object. A grasp taxonomy based on the contact web is also proposed as a tool to systematically identify a grasp. The grasp can be described at higher conceptual levels using a certain mapping function that results in an index called the grasp cohesive index. This index can be used to identify the grasp. Results from grasping experiments show that it is possible to distinguish between various types of grasps using the proposed contact web, grasp taxonomy and grasp cohesive index.},
author = {Kang, Sing Bing and Ikeuchi, Katsushi},
doi = {10.1109/70.246054},
file = {:home/ximon/school/thesis/doc/ref/toward-automatic-robot-instruction-from-perception{\_}recognizing-a-grasp-from-observation.pdf:pdf},
isbn = {1042-296X},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {4},
pages = {432--443},
title = {{Toward Automatic Robot Instruction from Perceptionâ€”Recognizing a Grasp from Observation}},
volume = {9},
year = {1993}
}
@article{Pandey2012,
abstract = {In a typical Human-Robot Interaction (HRI) scenario, the robot needs to perform various tasks for the human, hence should take into account human oriented constraints. In this context it is not sufficient that the robot selects grasp and placement of the object from the stability point of view only. Motivated from human behavioral psychology, in this paper we emphasize on the mutually depended nature of grasp and placement selections, which is further constrained by the task, the environment and the human's perspective. We will explore essential human oriented constraints on grasp and placement selections and present a framework to incorporate them in synthesizing key configurations of planning basic interactive manipulation tasks.},
author = {Pandey, Amit Kumar and Saut, Jean Philippe and Sidobre, Daniel and Alami, Rachid},
doi = {10.1109/BioRob.2012.6290776},
file = {:home/ximon/school/thesis/doc/ref/towards-planning-human-robot-interactive-manipulation-tasks.pdf:pdf},
isbn = {9781457711992},
issn = {21551774},
journal = {Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics},
pages = {1371--1376},
title = {{Towards planning Human-Robot Interactive manipulation tasks: Task dependent and human oriented autonomous selection of grasp and placement}},
year = {2012}
}
@article{Strabala2013,
abstract = {A handover is a complex collaboration, where actors coordinate in time and space to transfer control of an object. This coordination comprises two processes: the physical process of moving to get close enough to transfer the object, and the cognitive process of exchanging information to guide the transfer. Despite this complexity, we humans are capable of performing handovers seamlessly in a wide variety of situations, even when unexpected. This suggests a common procedure that guides all handover interactions. Our goal is to codify that procedure. To that end, we first study how people hand over objects to each other in order to understand their coordination process and the signals and cues that they use and observe with their partners. Based on these studies, we propose a coordination structure for human--robot handovers that considers the physical and social-cognitive aspects of the interaction separately. This handover structure describes how people approach, reach out their hands, and transfer objects while simultaneously coordinating the what, when, and where of handovers: to agree that the handover will happen (and with what object), to establish the timing of the handover, and to decide the configuration at which the handover will occur. We experimentally evaluate human-robot handover behaviors that exploit this structure and offer design implications for seamless human-robot handover interactions.},
author = {Strabala, Kyle Wayne and Lee, Min Kyung and Dragan, Anca Diana and Forlizzi, Jodi Lee and Srinivasa, Siddhartha and Cakmak, Maya and Micelli, Vincenzo},
doi = {10.5898/JHRI.2.1.Strabala},
file = {:home/ximon/school/thesis/doc/ref/towards-seamleass-human-robot-handovers.pdf:pdf},
isbn = {21630364},
issn = {21630364},
journal = {Journal of Human-Robot Interaction},
keywords = {handover,joint activity,physical human,robot interaction,signaling},
number = {1},
pages = {112--132},
title = {{Towards Seamless Human-Robot Handovers}},
url = {http://www.humanrobotinteraction.org/journal/index.php/HRI/article/view/114},
volume = {2},
year = {2013}
}
@article{Morales,
author = {Morales, A and Chinellato, E and Sanz, P J and Fagg, A H and Pobil, A P},
file = {:home/ximon/school/thesis/doc/ref/Vision{\_}based{\_}planar{\_}grasp{\_}synthesis{\_}and{\_}reliabilit.pdf:pdf},
title = {{Vision based planar grasp synthesis and reliability assessment for a multifinger robot hand : a learning approach}}
}
