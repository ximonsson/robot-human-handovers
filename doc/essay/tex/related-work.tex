% handovers and research around it
Robot-human handovers is a process that involves a lot parameters. Grasping of the object, social cues with their timing and head gaze are just a few of many. A lot of research has been made around identifying such parameters and their importance for a successful handover. Some research suggests that humans prefer a human-like motion when being handed over an object (\parencite{}), therefor other research tends to concentrate on observing humans perform handovers (\parencite{Moon2014}, \parencite{Huang2015}, \parencite{Admoni2014}). \textcite{Moon2014} found that only by imitating human gaze cues during a handover the robot can help the human reach for the object sooner, even before the robot finishes the movement. In \parencite{Huang2015} expirements were performed as to measure adaptation between humans in collaborative tasks, concluding that adaptevily coordinating actions can increase performs in tasks such as handovers. Although \textcite{Koene2014} measured through expirements the different levels of importance between spatial and temporal precision, finding that a fast motion from the robot was better recieved from the human side than a spatially accurate end position of the robotic grasp, \textcite{Admoni2014} found through their expirements that incoporating some deliberate delays in the handover help the reciever with focusing on the robot's head (i.e. gaze) and understand better it's suggestions which is suggested by \parencite{Moon2014}. Correct grasping of objects for handovers has sometimes been connected to the type of object and it's affordances (\parencite{Song2015}, \parencite{Chan2014}) where by associating an object with different ways of manipulating it we identify a grip which is to best suited when handing it over, as to increase productivity for the reciever. \textcite{Aleotti2012} developed a model based on user comfort for the reciever, in which the robot scans then environment and in a three dimensional reconstructed object model calculates the optimal handover by finding the best orientation of the object towards the human. \textcite{Chan2015} conducted a user study of handovers under different conditions and found that natural handovers may differ from reciever-oriented handovers, meaning that robots would need to take this into account when learning from demonstration to make sure it puts the human interests in priority.

% grasping
A very important feature to insure the success of robot-human handovers is the grasping of objects. Robotic grasping is itself a widely researched topic and several models for have been proposed to compute proper grasping for objects. \textcite{Miller2003} decompose an object into primitive shapes to identify good grasping points, and \textcite{Huebner2008} use a Minimum Volume Bounding Box solution to do the same.

% learning from unlabeled data and demonstration
One of the challenges of robot grasping is the ability to do grasping of objects that the robot has never seen before, nor been trained for. Some of the mentioned work does enable grasping of foreign objects, but requires a lot of data that has previously been labeled, for example the proposed model by \textcite{Chan2014} requires data on different ways to manipulate an object before the robot can calculate a way of grasping it and handing it over, making it hard to deploy in a new environment. Commonly in robot simulation is that data required for processing is represented by 3D models of the objects. \textcite{Saxena2008} use several images of an object from different angles, typically obtained through stereo camera, to identify grasping sites, no longer requiring a complete 3D model of the object in question. The training of the model does however require manually annotated 3D models. Complete and accurate 3D models are however not always available and not feasible in an environment where a robot should learn from observing humans. Kinect sensors have become popular within robotics today because of their relatively low cost and support for depth data. \parencite{Lenz2015} \parencite{Redmon2014} \parencite{Jiang2011} propose methods for identifying grasp sites using RGBD from a Kinect sensor.

% how to use this related work for this thesis
\textcite{Redmon2014} show very promising results with their implementation, illustrating the high performance of convolutional neural networks. The problem is that the data that it needs for training is manually annoted to recognize grasp sites on objects. \parencite{Chan2015a} propose a framework for robots to learn handovers and grasp sites through demonstration by handing objects to a robot, and then having it repeat the handover in reverse order back to the human. The problem with this framework is that there is no support for associating handovers to objects and use it on novel objects. This thesis will try and combine their work by implementing a framework for collecting data through demonstration from humans, process it as to train a convolutional neural network that can later identify grasping sites on novel objects by RGBD images.
