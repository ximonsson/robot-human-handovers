Robot-human handovers is a process that involves a lot parameters. Grasping of the object, social cues with their timing and head gaze are just a few of many. A lot of research has been made around identifying such parameters and their importance for a successful handover. Some research suggests that humans prefer a human-like motion when being handed over an object (\parencite{}), therefor other research tends to concentrate on observing humans perform handovers (\parencite{Moon2014}, \parencite{Huang2015}, \parencite{Admoni2014}). \textcite{Moon2014} found that only by imitating human gaze cues during a handover the robot can help the human reach for the object sooner, even before the robot finishes the movement. In \parencite{Huang2015} expirements were performed as to measure adaptation between humans in collaborative tasks, concluding that adaptevily coordinating actions can increase performs in tasks such as handovers. Although \textcite{Koene2014} measured through expirements the different levels of importance between spatial and temporal precision, finding that a fast motion from the robot was better recieved from the human side than a spatially accurate end position of the robotic grasp, \textcite{Admoni2014} found through their expirements that incoporating some deliberate delays in the handover help the reciever with focusing on the robot's head (i.e. gaze) and understand better it's suggestions which is suggested by \parencite{Moon2014}.

A very important feature to insure the success of robot-human handovers is the grasping of objects. Robotic grasping is itself a widely researched topic and several models for have been proposed to compute proper grasping for objects (\parencite{Miller2003} ...). Correct grasping can sometimes be connected to the type of object and it's affordances (\parencite{Song2015}, \parencite{Chan2014}). One of the challenges of robot grasping is the ability to do grasping of objects that the robot has never seen before, nor been trained for. Grasping novel objects has been researched previously (\parencite{Saxena2008} \parencite{Lenz2015} \parencite{Redmon2014} ...).

The difficulty with the work in \parencite{Saxena2008} ... rely on having a complete 3D-model of the object to be grasped for the robot. These models are not trivial to come by and difficult to generate in a real-time scenario where we would like the robot to learn by observations. In \parencite{Lenz2015} \parencite{Redmon2014} \parencite{Jiang2011} they have instead used data from Kinect sensor which are a good solution for robots that process data in real-time from their environment.

\parencite{Redmon2014} show very promising results with their implementation, illustrating the high performance of convolutional neural networks. The problem is that the data that it needs for training is manually annoted to recognize grasp sites on objects. \parencite{Chan2015a} propose a framework for robots to learn handovers and grasp sites through demonstration by handing objects to a robot, and then having it repeat the handover in reverse order back to the human. The problem with this framework is that there is no support for associating handovers to objects and use it on novel objects.
