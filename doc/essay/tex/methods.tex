The first part of the work will be to observe a number of handovers between humans for a set of objects and try to categorise the observed handovers into different classes. In a second part each object will then be assigned a class that we train a machine to return when given an image.

Seeing how we do not have any labels for the handovers, but still want to train a model that outputs some defined class for how the object should be handed over, we will be using a semi-supervised approach in this project. By first applying an unsupervised learning algorithm to categorise and create classes of handovers that are later used as output through supervised learning.

\section{Tools and material}

\subsection*{Hardware}

During the project the following material were used:
\begin{description}
	\item[Kinect v2] for recording and taking pictures with both color channels and depth data.
	\item[\textcite{AprilTags}] used for object detection and tracking.
	\item[Fifteen different objects] Including: a knife, a screwdriver, a can, a box, a pair of cutters, a tube, a pitcher, a glass, a bottle, a brush, a cup, a hammer, a pen, a scalpel and a pair of scissors.
\end{description}

\subsection*{Software}

The applications will be written mostly in C++ or Python depending on availability of libraries. To help implement our system we will have the following programs and programming libraries at our disposal:
\begin{description}
	\item[\textcite{GIMP}] For preprocessing of some images manually.
	\item[\textcite{libfreenect2}] Open source library written in C++ for communicating with the Kinect camera.
	\item[\textcite{OpenCV}] Open source library for computer vision with bindings for C++ and Python, and other languages.
	\item[AprilTags programming library] for detecting tags within captured images, and extracting data.
\end{description}


\section{Categorisation of handovers}

\subsection{Collection of data}

For this purpose a number of voluteers were asked to be recorded while handing over a set of objects between each other, using a \emph{Kinect v2} the \textcite{libfreenect2} software library. Even though the camera used supports depth images, only two-dimensional data will be used in these part, i.e we will only use the color images recorded. A total of 11 people who already knew each other from before volunteered (including myself) for this task.  Pairs of volunteers were permutted to try and make givers adapt to different recievers. 15 objects were used in the recordings including: \emph{kitchen knife, hammer, screwdriver, brush, pen, scissors, bottle, cup, pitcher, glass, ball, tube, box, can, scalpel}. Instructions for handing over the objects were given prior to recordings as to make sure participants gave the object in a manner that the reciever can used the object straight away, for example give the hammer handle first in a manner that the next person can start hammering directly without requiring a switch of grip after recieving, or a cup is to handed handle first so the person can start drinking right away. For objects that might contain liquids the participants were asked to imagine that these objects were filled while handing them over between each other. Also as to make sure the dataset did not contain too much deviations considering the grasp some instructions were given concerning region were to grasp, for example that the giver grasps the bottle in the top half. The regions were chosen by myself with in mind for optimization for the reciever when given the object.

In order to extract data from the recorded frames about the objects we need to detect the objects themselves. Object detection is a very difficult part of computer vision and there are many different methods, where some are more robust than others, to do so. To help with this part to make data extraction easier we will be using AprilTags. AprilTags can be compared to QR-codes that are printed out on small papers and fastened onto the objects. With the help of their software library we can easily track the objects in the recorded material by getting the position of the four corners of the tag, and its center, which enables us to estimate such things as pose of the object. More can be learned about AprilTags at \parencite{AprilTags}.

For each object we manually first create a reference image with its ground truth and from this image we also generate a binary mask. Figure \ref{fig:objects_montage} shows the reference images created and figure \ref{fig:masks_montage} show their binary masks. These images were created using the \textcite{GIMP} - GIMP. With the help of the AprilTags we can compute a Homography matrix that would warp the tag (and object) in the reference image to the one located in the frame with the detected object. The binary mask of the image is transformed using the computed Homography matrix and applied over the scene to extract only the entire object itself, and with this we can estimate the grasping region. The image of the found object is segmentated by color, skin color more precisely in this case, and a contour expressed as rotated rectangle is calculated around the largest region found which represents our grasping region. Skin color is a quick way of segmentating and finding grasp region as the color of skin is quite different from the colors of the objects, but the method is not too robust in covering many different people as the color can vary quite a lot from person to person. The setting for skin color had to therefore be tweaked manually depending on the participant.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/methods/objects.jpg}
	\caption{Ground truth images of the objects used with an AprilTag fastened to them}
	\label{fig:objects_montage}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/methods/masks.jpg}
	\caption{Binary mask of each object}
	\label{fig:masks_montage}
\end{figure}

One recording session consists from one pair of volunteers, the giver has handed over all objects in the set to the reciever. During the recording the application is continuously recording, creating a large number frames that are not of interest. In this project we only interest ourselves for the actual moment that the giver holds out the object ready for the reciever to take it, which requires some post-processing of the frames. A first step is to create a Region of Interest (ROI) which we will concentrate on searching for objects. Our ROI is arbitrarely set to the middle of the frame and large enough to see the entire object. When a tag is detected within the the ROI we flag this frame a handover moment and store the features of the handover. This is unfortunately not enough because we might be flagging frames where the reciever is holding the object after taking it, or maybe both are holding it but the system detects the reciever's hand as the largest grasping region. Figures \ref{fig:handover_bottle} and \ref{fig:handover_hammer} demonstrate frames that the system detected as handovers with the objects masked out and the grasping region identified. The methods explained above might also be incorrect as the grasping region was incorrectly detected. Manual filtering was later applied to images that were flagged as handover moments and to make sure they were correct ones and that the data extracted was correct. Figure \ref{fig:handover_incorr} illustrates a situation that needs to be manually filtered out when the system percieves to have identified a handover situation but the grasping region identified is that of the reciever's.

\begin{figure}
	\centering
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/methods/handovers/bottle_frame.jpg}
		\caption{Handover of bottle}
		\label{fig:demo_handover_bottle}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/methods/handovers/bottle_masked.jpg}
		\caption{Bottle masked out and grasp region identified.}
		\label{fig:handover_bottle_masked}
	\end{subfigure}
	\caption{Handover demonstation of a bottle with the grasp region detected after being masked out. The giver was instructed to pretend like the bottle was full of liquids to make sure they held it upright.}
	\label{fig:handover_bottle}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/methods/handovers/hammer_frame.jpg}
		\caption{Handover of hammer}
		\label{fig:demo_handover_hammer}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/methods/handovers/hammer_masked.jpg}
		\caption{Hammer masked out and grasp region identified.}
		\label{fig:handover_hammer_masked}
	\end{subfigure}
	\caption{Handover demonstration of a hammer with the grasp region detected after being masked out. The giver was instructed to give it handle first, holding the head so the reciever is able to use it straight away.}
	\label{fig:handover_hammer}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/methods/handovers/incorr_frame.jpg}
		\caption{Handover of the hammer after the reciever takes the object.}
		\label{fig:demo_handover_incorr}
	\end{subfigure}
	\par\bigskip
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/methods/handovers/incorr_mask.jpg}
		\caption{Hammer with the grasp region of the reciever identified.}
		\label{fig:handover_incorr_masked}
	\end{subfigure}
	\caption{Example of frame with false positive. The object is detected within the region of interest but the identified grasping region is not that of the giver, but the reciever's.}
	\label{fig:handover_incorr}
\end{figure}


\subsection{Clustering by handover features}

After observing a number of handovers betweens humans, a first step is to try and categorise these handovers into different classes that can be applied for the objects. As we do not have any labels, or even unsure how many different categories of handovers we have recorded, we will attempt at clustering using an unsupervised learning algorithm by the features that we have extracted from the recorded material.

\subsubsection{K-Means}

For the purpose of categorizing the handover samples we will be using the \emph{k-means} algorithm. K-means is an algorithm that attempts to classify a number of observations into \emph{k} number of clusters. The algorithm starts by creating \emph{k} number of centroids for each cluster and assigning our samples to the centroid within shortest distance. The algorithm iteratively then updates the centroids to become the mean of the samples assigned to it, and then re-assigns all samples depending on the new centroids. This is repeated either a fixed number of times or until the algorithm converges minimizing the sum of squares within the clusters.

\subsubsection{Principle component analysis (PCA)}

A challenge that presents itself using clustering algorithms is choosing what features are to be used for the purpose and how many clusters should we create. We will be using \emph{Principle component analysis} (PCA) to evaluate which features that are to be used to define each cluster. Ideally we use the smallest number of features possible, but also the ones that create the most distinct clusters, which PCA will help us with. Before using techniques such as PCA to minimize the amount of features that are needed to cluster we will try to define what features matter in a handover and deduct them from the data we have, as well as trying to redefine some features in a more compressed matter.

\subsubsection{Extracting features}

As we mentioned we are able to extract the homography matrix the grasping region expressed as a rotated rectange from the recorded data. From these we can derive a number of features that might be of interest to classify a handover. A homography matrix \(H\) is a 3x3 matrix with a total of nine values which creates quite a few features if we would try and cluster on it. Instead we can use it to calculate a rotation of the object, we discard the translation information in the matrix as we are only interested in the end pose of the object. As we are only working with two-dimensional data we will use the rotation on the z-axis. Using the homography we can calculate a rotation matrix from which we can derive the angle. Using the following Python in \ref{lst:rotation_matrix}, code with the help of the OpenCV library we compute a rotation matrix \(R\) (taken from \parencite{OpenCVHomographyDemo}). Using the rotation matrix we can compute the angle \(\theta_Z\) as \(\theta_Z = \arctan(R_{1,0} / R_{0,0})\).

\begin{lstlisting}[language=Python,frame=single,label={lst:rotation_matrix},caption={Python code to calculate rotation matrix from homography matrix using Opencv and numpy libraries}]
norm = np.sqrt(np.sum(H[:,0] ** 2))
H /= norm
c1 = H[:, 0]
c2 = H[:, 1]
c3 = np.cross(c1, c2)

R = np.zeros((3, 3), dtype=np.float64)
for i in range(3):
	R[i, :] = [c1[i], c2[i], c3[i]]
w, u, t = cv2.SVDecomp(R)
return np.dot(u, t)
\end{lstlisting}

Next we can compute a center of mass for the object using it's contour's moments, easily extracted from it's binary mask. The following formula gives us the center. \textcolor{red}{write about moments and correct formula for center}

\[
	center = \frac{M_x}{M}, \frac{M_y}{M}
\]

We can then extract the distance between center of mass of the object and the center of the grasping region as a feature. It also enables us to create a unit vector from the center of mass of the object towards the grasping region. A vector in two dimensions would be expressed with two values, but we can summarize this in one value by instead calculating the angle using \(\tan(\theta) = \sqrt{x^2 + y^2}\). Lastly we can compute the ratio of the area covered by the grasp by the area of the object \(A = \frac{area(Grasp)}{area(Object}\).

To summarize we can extract the following features: \textcolor{red}{make the below lists better}
\begin{itemize}
	\item Rotation of object in z-axis, extracted from the homography matrix of the AprilTag.
	\item Distance between center of mass of the object and the center of the grasping region.
	\item Angle between the two mentioned centers.
	\item Ratio of area of the grasping region by the area of object.
\end{itemize}

On top of these we still have:
\begin{itemize}
	\item Width of grasp region
	\item Height of grasp region
	\item Angle of grasp region
	\item Center of grasp region
	\item Center of mass of the object
\end{itemize}

With these features we define for a robot how to grasp an object and present it to a human for a handover. More on the role of the features for clustering will be presented in the results and discussion section.


\section{Prediction of handover class}

Our aim is in the end is to for given an image of an object, have a robot be able to predict a handover setting. In our case we will have it predict a class of handovers which are the result of the previous clustering. For novel objects we will instead output the class of the object among the training set that it resembles the most.

\subsection{Convolutional Neural Networks}

We hypothesis that there are a number of features with the objects that can be retrieved visually that define which class of handovers it belongs to. Even though we could try and preprocess all sets of objects and re-define them as a set of features, choosing which features it should be and how to retrieve them would be a challenging task, especially before knowing the results of the clustering. In \ref{sec:machine_learned_systems} we mentioned convolutional neural networks and works such as \parencite{Lee2009}, \parencite{Turaga2010} where the layers learn to extract features in the images to perform visual tasks. This ability of CNNs corresponds well with our goal to have the machine learn anatonomously without human preprocessing or labeling.

\subsection{AlexNet network}

For the task of predicting handover class from an image we will be training a CNN. A challenging part with all artificial neural networks is to define their architecture. Changes to the number of layers and their sizes can play a significant role on the outcome of the results. Within object classification there have been a number of architectures proposed that have achieved good results, one of these is the \texttt{AlexNet} network which was mentioned in \ref{sec:machine_learned_systems}, which we will use as basis for our own network.

The AlexNet was designed for the ImageNet object classification problem in 2010, trained over 1,2 million images to output 1000 different classes.
As a reminder, figure \ref{fig:alexnet_orig} shows the network architecture used by the AlexNet. It includes in total eight layers, of which every uses the ReLU non-linearity function:

\begin{description}
	\item[Five convolutional layers] The first convolutional layer filters the 224 x 224 x 3 input image with 96 kernels of size 11x11x3 with a stride of 4 pixels. The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 x 5 x 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 x 3 x 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 x 3 x 192 , and the fifth convolutional layer has 256 kernels of size 3 x 3 x 192.
	\item[Three fully connected layers] with 4096 neurons each, where the last one is fed to a 1000-way softmax output.
\end{description}

In total the network consists of 60 million parameters and 650000 neurons. At the time, the designers of the network were under the restriction of the performance of GPUs, hence the parallell system. Seeing how our classification problem is of significantly smaller scale it's size will do without problem, note however that we will re-design it so it runs sequentially on one GPU. The only other modification we will attempt will be by adding a new fully connected layer at the end with size depending on the results from the prior clustering. Figure \ref{fig:method_cnn_arch} shows our network architecture. As a note, since the AlexNet network was designed, several other successful network architectures have been proposed, some that even outperform AlexNet, such as \texttt{VGG-16} by \textcite{Arge2015} and \texttt{Inception} \textcite{Szegedy2014}. These networks are however larger and deeper which would in theory require more processing time and as well as more data to train, both which are of limited supply. The AlexNet therefor makes a good compromise.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/related-work/cnn-architecture.png}
	\caption{\textcolor{red}{insert correct graph}}
	\label{fig:method_cnn_arch}
\end{figure}

\subsection{Training and testing}

The goal is to have the machine learn from the handovers between humans that it has observed. We will therefor use the objects that were used for recording handovers as our dataset for this purpose as well. Even though their potential for performing very well, one issue with ANNs is that they often require a lot of data to do so. The amount of data we have been able to gather by taking images of the objects are unfortunately not enough for a network as large as the AlexNet network. Fortunately for us there are pre-trained weights to be found that we can use as to increase our chances. For example we can use the ones found at \parencite{AlexNetImplWeights} where we can also base our implementation of the AlexNet architecture using the Python library \texttt{tensorflow}.  Next step is to finetune the pre-trained weights to better fit our own data. More precisely we will train it to output one of the labels created through the clustering instead one the one thousand classes it was originally trained for. As input data we will be taking inspiration from \parencite{Redmon2014} and perform the same alterations to our own dataset by replacing the blue color channel with the depth image. Figure \ref{fig:rgbd} shows an example of the hammer with it's RGB image to the left, depth image in the middle and to the right the color image with the blue channel replaced for the depth data.

Using a Kinect v2 camera, both color and depth images were taken of all objects from several different angles in different positions. Data augmentation was later performed on the resulting images to enlargen the dataset by using flipping, random cropping and random rotations of the images. There are issues like black triangles in the corners or along the edges after rotating and/or cropping. These images are unwanted as the network might identify these regions with objects. Therefor all the images created through data augmentation will be gone through manually as to make sure they are all suitable for processing by the network. After we are done selecting images for our dataset we are not guaranteed that it is balanced between the objects or the classes. To prevent any overfitting we will also make sure that we balance the dataset evenly over all objects and classes.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{img/methods/rgbd/rgb.jpg}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{img/methods/rgbd/depth.jpg}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{img/methods/rgbd/rgbd.jpg}
	\end{subfigure}
	\caption{RGB-, depth- and RGB-image with blue channel replaced by the depth data for the hammer.}
	\label{fig:rgbd}
\end{figure}

Training and validation will be performed on a larger subset of the images to see how well the system is able to recognize and predict a class for objects it has been trained for. While a smaller subset will be used later for testing to see how well the system performs on foreign objects that it has never seen before. The sizes of these subsets will be varied as to see how many objects are required for training to perform well on new objects.
